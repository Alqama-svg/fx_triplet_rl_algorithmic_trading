{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "75e7279e-9150-462e-91c8-bbb9fe583e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from envs.fx_triplet_env import FXTripletEnv, FXTripletConfig\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except Exception as e:\n",
    "    raise ImportError(\"PyTorch is required for this DDQN implementation.\"\n",
    "        \"Install it before running this cell. Example (CPU-only):\\n\"\n",
    "        \"  pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\\n\"\n",
    "        \"or see https://pytorch.org/get-started/locally/ for the correct command for environment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81502485-cfdb-45c7-a662-f3b6cb325601",
   "metadata": {},
   "source": [
    "## Config for DDQN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f4c0eccd-ceaf-4ba3-bdc7-14de518df9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DDQNConfig:\n",
    "    # trading\n",
    "    seed: int = 0\n",
    "    total_episodes: int = 500\n",
    "    max_steps_per_episode: int = 10\n",
    "    batch_size: int = 64\n",
    "    buffer_size: int = 10000\n",
    "    gamma: float = 0.999\n",
    "    lr: float = 1e-4\n",
    "    target_update_freq: int = 100      # in gradient steps\n",
    "    start_training_after: int = 500    # steps to fill replay before training\n",
    "    train_frequency: int = 1           # train every N steps\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_final: float = 0.01\n",
    "    epsilon_decay_steps: int = 20000   # linear decay steps\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_path: str = \"./ddqn_checkpoints\"\n",
    "    log_dir:str = \"./ddqn_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bca20b-915c-43f6-b653-e0e371dead69",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "35073766-cd8d-4d71-85a4-8f120752b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, seed: Optional[int] = None):\n",
    "        self.capacity = int(capacity)\n",
    "        self.buffer = deque(maxlen = self.capacity)\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = self.rng.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0acd2-095f-43e0-ac46-ef3eb35e6fdb",
   "metadata": {},
   "source": [
    "## Q-network (simple MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bf7e2831-ee3a-4205-8568-89da16f1e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_sizes: List[int] = [64,64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, action_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608be756-8740-429c-9431-5898adbbcec8",
   "metadata": {},
   "source": [
    "## DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "75c6a2fc-00e8-4e3a-ab8d-6174588054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 cfg: DDQNConfig,\n",
    "                 hidden_sizes: List[int] = [64, 64],\n",
    "                 discrete_action_table: Optional[np.array] = None):\n",
    "        \"\"\"\n",
    "        env: gym-like env implementing Step 1\n",
    "        cfg: DDQNConfig dataclass\n",
    "        discrete_action_table: if env uses discrete actions, provide mapping array (n_actions, 3)\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        self.device = torch.device(cfg.device)\n",
    "        # determine state / action dims\n",
    "        self.state_dim = int(np.prod(env.observation_space.shape))\n",
    "        # For discrete actions, the agent acts over ints 0..n-1\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            # discrete\n",
    "            self.discrete = True\n",
    "            self.n_actions = int(env.action_space.n)\n",
    "            self.action_dim = self.n_actions\n",
    "            if discrete_action_table is None:\n",
    "                self.discrete_action_table = getattr(env, \"_action_table\", None)\n",
    "            else:\n",
    "                self.discrete_action_table = discrete_action_table\n",
    "        else:\n",
    "            self.discrete = False\n",
    "            # continuous actions: discretize to N bins per dimension for DDQN baseline if needed\n",
    "            # But paper uses DDQN with a small discrete action set; here I assume env.action_space is Discrete for DDQN.\n",
    "            raise ValueError(\"DDQN baseline expects a discrete action space in the env. for continous, discrete first\")\n",
    "        # networks\n",
    "        self.policy_net = MLPNetwork(self.state_dim, self.action_dim, hidden_sizes).to(self.device)\n",
    "        self.target_net = MLPNetwork(self.state_dim, self.action_dim, hidden_sizes).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # optimizer & buffer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "        self.replay = ReplayBuffer(cfg.buffer_size, seed=cfg.seed)\n",
    "        self.total_steps = 0\n",
    "        self.train_steps = 0\n",
    "\n",
    "        # epsilon schedule params\n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.epsilon_decay = (cfg.epsilon_start - cfg.epsilon_final) / float(max(1, cfg.epsilon_decay_steps))\n",
    "\n",
    "        # logging\n",
    "        os.makedirs(cfg.save_path, exist_ok=True)\n",
    "        os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "        self.writer = SummaryWriter(cfg.log_dir)\n",
    "\n",
    "    def select_action(self, state: np.ndarray, eval_mode: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        state: np.ndarray shaped like env.observation_space\n",
    "        returns: integer action (index into discrete action table)\n",
    "        \"\"\"\n",
    "        if not eval_mode and self.rng.random() < self.epsilon:\n",
    "            return int(self.rng.integers(0, self.action_dim))\n",
    "        # else greedy\n",
    "        state_t = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.policy_net(state_t)\n",
    "            action = int(torch.argmax(qvals, dim = 1).cpu().item())\n",
    "        return action\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state.astype(np.float32), int(action), float(reward), next_state.astype(np.float32), bool(done))\n",
    "\n",
    "    def _compute_td_loss(self, batch: Transition) -> torch.Tensor:\n",
    "        states = torch.tensor(np.stack(batch.state), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(batch.action, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch.reward, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.stack(batch.next_state), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(batch.done, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        # current Q(s, a)\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        # Double DQN target: I used policy_net to choose argmax action, target_net to eval\n",
    "        with torch.no_grad():\n",
    "            next_q_policy = self.policy_net(next_states)\n",
    "            next_actions = torch.argmax(next_q_policy, dim=1, keepdim=True)\n",
    "            next_q_target = self.target_net(next_states).gather(1, next_actions)\n",
    "            td_target = rewards + (1.0 - dones) * (self.cfg.gamma * next_q_target)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, td_target)\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self):\n",
    "        # ensuring enough samples\n",
    "        if len(self.replay) < max(self.cfg.batch_size, 1):\n",
    "            return None\n",
    "        batch = self.replay.sample(self.cfg.batch_size)\n",
    "        loss = self._compute_td_loss(batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping for stability\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        self.train_steps += 1\n",
    "        \n",
    "        # target network update\n",
    "        if (self.train_steps % self.cfg.target_update_freq) == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return float(loss.item())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.cfg.epsilon_final:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            if self.epsilon < self.cfg.epsilon_final:\n",
    "                self.epsilon = self.cfg.epsilon_final\n",
    "    \n",
    "    def save(self, tag: str = \"latest\"):\n",
    "        path = os.path.join(self.cfg.save_path, f\"ddqn_{tag}.pth\")\n",
    "        torch.save({\n",
    "            \"policy_state_dict\": self.policy_net.state_dict(),\n",
    "            \"target_state_dict\": self.target_net.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"cfg\": self.cfg,\n",
    "        }, path)\n",
    "        print(f\"Saved model to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "        self.policy_net.load_state_dict(data[\"policy_state_dict\"])\n",
    "        self.target_net.load_state_dict(data[\"target_state_dict\"])\n",
    "        if \"optimizer_state_dict\" in data:\n",
    "            try:\n",
    "                self.optimizer.load_state_dict(data[\"optimizer_state_dict\"])\n",
    "            except Exception:\n",
    "                print(\"Could not load optimizer state (shapes mismatch). Continuing without loading optimizer state\")\n",
    "        print(f\"Loaded model from {path}\")\n",
    "    \n",
    "    def action_index_to_env_action(self, index: int) -> np.ndarray:\n",
    "        if self.discrete_action_table is None:\n",
    "            raise RuntimeError(\"No discrete action table available to map index to environment action\")\n",
    "        return np.array(self.discrete_action_table[int(index)], dtype=np.float32)\n",
    "\n",
    "    # Full training loop (episodic)\n",
    "    def train(self,\n",
    "              save_every_episodes: int = 50,\n",
    "              eval_every_episodes: int = 50,\n",
    "              eval_episodes: int = 10,\n",
    "              verbose: bool = True):\n",
    "        total_steps = 0\n",
    "        episode_rewards = []\n",
    "        for episode in range(1, self.cfg.total_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            ep_reward = 0.0\n",
    "            for step in range(self.cfg.max_steps_per_episode):\n",
    "                # action index selection\n",
    "                action_idx = self.select_action(state, eval_mode=False)\n",
    "                # map to env action values\n",
    "                env_action = self.action_index_to_env_action(action_idx)\n",
    "                next_state, reward, done, info = self.env.step(env_action)\n",
    "                self.push_transition(state, action_idx, reward, next_state, done)\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "                total_steps += 1\n",
    "                # training step\n",
    "                if total_steps >= self.cfg.start_training_after and (total_steps % self.cfg.train_frequency == 0):\n",
    "                    loss = self.train_step()\n",
    "                    if loss is not None:\n",
    "                        self.writer.add_scalar(\"train/loss\", loss, global_step=total_steps)\n",
    "                # epsilon decay\n",
    "                self.decay_epsilon()\n",
    "                if done:\n",
    "                    break\n",
    "            episode_rewards.append(ep_reward)\n",
    "            self.writer.add_scalar(\"episode/reward\", ep_reward, episode)\n",
    "            if verbose and (episode % 10 == 0 or episode == 1):\n",
    "                avg_recent = np.mean(episode_rewards[-50:])\n",
    "                print(f\"Episode {episode}/{self.cfg.total_episodes}   ep_reward={ep_reward:.4f}  eps={self.epsilon:.4f}  avg50={avg_recent:.4f}\")\n",
    "            # save\n",
    "            if episode % save_every_episodes == 0:\n",
    "                self.save(tag=f\"ep{episode}\")\n",
    "            # eval\n",
    "            if episode % eval_every_episodes == 0:\n",
    "                avg_eval = self.evaluate(n_episodes=eval_episodes)\n",
    "                self.writer.add_scalar(\"eval/avg_reward\", avg_eval, episode)\n",
    "        # final save\n",
    "        self.save(tag=\"final\")\n",
    "        return episode_rewards\n",
    "\n",
    "\n",
    "    # Evaluation (greedy policy)\n",
    "    def evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0  # greedy\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                a = self.action_index_to_env_action(idx)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b4cb7-9f21-43d6-b7d5-851d0a897cd1",
   "metadata": {},
   "source": [
    "## Full training loop (episodic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "55ef56b5-a181-417f-ab73-41a3f1526b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,\n",
    "        save_every_episodes: int = 50,\n",
    "        eval_every_episodes: int = 50,\n",
    "        eval_episodes: int = 10,\n",
    "        verbose: bool = True):\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    for episode in range(1, self.cfg.total_episodes + 1):\n",
    "        state = self.env.reset()\n",
    "        ep_reward = 0.0\n",
    "        for step in range(self.cfg.max_steps_per_episode):\n",
    "            # action index selection\n",
    "            action_idx = self.select_action(state, eval_mode=False)\n",
    "            # map to env action values\n",
    "            env_action = self.action_index_to_env_action(action_idx)\n",
    "            next_state, reward, done, info = self.env.step(env_action)\n",
    "            self.push_transition(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            total_steps += 1\n",
    "            # training step\n",
    "            if total_steps >= self.cfg.start_training_after and (total_steps % self.cfg.train_frequency == 0):\n",
    "                loss = self.train_step()\n",
    "                if loss is not None:\n",
    "                    self.writer.add_scalar(\"train/loss\", loss, global_step=total_steps)\n",
    "            # epsilon decay\n",
    "            self.decay_epsilon()\n",
    "            if done:\n",
    "                break\n",
    "        episode_rewards.append(ep_reward)\n",
    "        self.writer.add_scalar(\"episode/reward\", ep_reward, episode)\n",
    "        if verbose and (episode % 10 == 0 or episode == 1):\n",
    "            avg_recent = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode}/{self.cfg.total_episodes}   ep_reward={ep_reward:.4f}  eps={self.epsilon:.4f}  avg50={avg_recent:.4f}\")\n",
    "        # save\n",
    "        if episode % save_every_episodes == 0:\n",
    "            self.save(tag=f\"ep{episode}\")\n",
    "        # eval\n",
    "        if episode % eval_every_episodes == 0:\n",
    "            avg_eval = self.evaluate(n_episodes=eval_episodes)\n",
    "            self.writer.add_scalar(\"eval/avg_reward\", avg_eval, episode)\n",
    "        # final save\n",
    "        self.save(tag=\"final\")\n",
    "        return episode_rewards\n",
    "\n",
    "    # Evaluation (greedy policy)\n",
    "    def evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0  # greedy\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                a = self.action_index_to_env_action(idx)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf20a6-886d-4aa0-99e0-e924629b4690",
   "metadata": {},
   "source": [
    "## Evaluation (greedy policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9faa3f97-66ef-4af2-b79a-6bad4bbdefe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\\n        old_eps = self.epsilon\\n        self.epsilon = 0.0  # greedy\\n        rewards = []\\n        for ep in range(n_episodes):\\n            s = self.env.reset()\\n            ep_r = 0.0\\n            for _ in range(self.cfg.max_steps_per_episode):\\n                idx = self.select_action(s, eval_mode=True)\\n                a = self.action_index_to_env_action(idx)\\n                s, r, done, _ = self.env.step(a)\\n                ep_r += r\\n                if render:\\n                    self.env.render()\\n                if done:\\n                    break\\n            rewards.append(ep_r)\\n        self.epsilon = old_eps\\n        avg = float(np.mean(rewards))\\n        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\\n        return avg'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0  # greedy\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                a = self.action_index_to_env_action(idx)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930aecc-7f43-46df-bf88-84baf519c0d1",
   "metadata": {},
   "source": [
    "## creation of an action table for DDQN using env._action_table as per availablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "72dcf477-d2ff-43ea-abbc-2c784e420e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discrete_table_from_env(env, max_grid_per_dim=5):\n",
    "    \"\"\"\n",
    "    If env has an _action_table attribute, return it.\n",
    "    Otherwise, build a coarse grid over 3 dims with max_grid_per_dim values per dimension.\n",
    "    \"\"\"\n",
    "    if hasattr(env, \"_action_table\") and env._action_table is not None:\n",
    "        return np.array(env._action_table)\n",
    "    # try to inspect action bounds\n",
    "    assert hasattr(env, \"cfg\"), \"Env must expose cfg with min_trade/max_trade\"\n",
    "    minv = env.cfg.min_trade\n",
    "    maxv = env.cfg.max_trade\n",
    "    grid = np.linspace(minv, maxv, max_grid_per_dim)\n",
    "    combos = np.array(np.meshgrid(grid, grid, grid)).T.reshape(-1,3)\n",
    "    # optionally prune duplicates or too many combos\n",
    "    # choose a subset\n",
    "    n = min(len(combos), 125)  # cap to 125\n",
    "    idx = np.linspace(0, len(combos)-1, n).astype(int)\n",
    "    return combos[idx].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366aac74-76ba-44b5-890f-9776e35189ed",
   "metadata": {},
   "source": [
    "## Smoke-test routine (small run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0ba7f466-867e-4ee9-8b44-fb8c513f9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_smoke_test():\n",
    "    print(\"Running DDQN smoke test (very small training run)...\")\n",
    "    # create env if not in global scope\n",
    "    try:\n",
    "        env  # check if env exists\n",
    "    except NameError:\n",
    "        # require user to define env in notebook\n",
    "        raise RuntimeError(\"Please ensure FXTripletEnv from Step 1 is defined and 'env' is instantiated in the notebook scope before running this smoke test.\")\n",
    "    # wrap up\n",
    "    cfg = DDQNConfig(total_episodes=20, max_steps_per_episode=env.cfg.T, buffer_size=2000, start_training_after=20, epsilon_decay_steps=1000)\n",
    "    disc_table = make_discrete_table_from_env(env, max_grid_per_dim=3)\n",
    "    agent = DDQNAgent(env, cfg, hidden_sizes=[64,64], discrete_action_table=disc_table)\n",
    "    # run training for a few episodes\n",
    "    rewards = agent.train(save_every_episodes=10, eval_every_episodes=10, eval_episodes=3, verbose=True)\n",
    "    print(\"Smoke test completed. Sample rewards:\", rewards[:5])\n",
    "    return agent, rewards\n",
    "# agent, rewards = ddqn_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1fa26e30-8b38-4297-a9d4-6618e2a8f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/20   ep_reward=-20.7218  eps=0.9901  avg50=-20.7218\n",
      "Episode 10/20   ep_reward=-6.2546  eps=0.9010  avg50=-16.5195\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep10.pth\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DDQNAgent' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m cfg \u001b[38;5;241m=\u001b[39m DDQNConfig(\n\u001b[0;32m      9\u001b[0m     total_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[0;32m     10\u001b[0m     max_steps_per_episode\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mT, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     epsilon_decay_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDQNAgent(env, cfg, hidden_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_every_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n",
      "Cell \u001b[1;32mIn[160], line 182\u001b[0m, in \u001b[0;36mDDQNAgent.train\u001b[1;34m(self, save_every_episodes, eval_every_episodes, eval_episodes, verbose)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# eval\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m eval_every_episodes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m         avg_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(n_episodes\u001b[38;5;241m=\u001b[39meval_episodes)\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval/avg_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_eval, episode)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# final save\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DDQNAgent' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "cfg_env = FXTripletConfig(\n",
    "    seed=0,\n",
    "    discrete_action=True,\n",
    "    n_discrete=125\n",
    ")\n",
    "env = FXTripletEnv(cfg_env)\n",
    "\n",
    "cfg = DDQNConfig(\n",
    "    total_episodes=20, \n",
    "    max_steps_per_episode=env.cfg.T, \n",
    "    buffer_size=2000, \n",
    "    start_training_after=20, \n",
    "    epsilon_decay_steps=1000\n",
    ")\n",
    "\n",
    "agent = DDQNAgent(env, cfg, hidden_sizes=[64,64])\n",
    "rewards = agent.train(save_every_episodes=10, eval_every_episodes=10, eval_episodes=3, verbose=True)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d5ec8-3bcb-4466-b1b1-a6eccd56b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_env = FXTripletConfig(seed=0, discrete_action=True, n_discrete=125)\n",
    "env = FXTripletEnv(cfg_env)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a933ce-a8cc-4399-8c00-a9d86c290d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(FXTripletConfig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f87521-af83-492c-9d0e-8da5b20d4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_env = FXTripletConfig(seed=0, discrete_action=True, n_discrete=125)\n",
    "env = FXTripletEnv(cfg_env)\n",
    "\n",
    "print(\"Config discrete_action:\", cfg_env.discrete_action)\n",
    "print(\"Env discrete_action:\", env.discrete_action)\n",
    "print(\"Env action_space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d35fc8-5ba7-403a-8d51-0d69a45630b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inside ddqn_baseline.ipynb:\")\n",
    "print(\"Env action space:\", env.action_space)\n",
    "print(\"Type:\", type(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d79092-9e8b-4b8f-9102-88bbb9ec9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "print(\"Action space object:\", env.action_space)\n",
    "print(\"Isinstance check:\", isinstance(env.action_space, gym.spaces.Discrete))\n",
    "print(\"Type of action space:\", type(env.action_space))\n",
    "print(\"gym.spaces.Discrete class:\", gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af41d36-8d3e-40a2-a11c-3cc376a5b4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
