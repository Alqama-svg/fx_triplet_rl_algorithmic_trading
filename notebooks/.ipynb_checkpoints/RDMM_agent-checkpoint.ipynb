{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91084dc0-02a5-4d33-bb35-972781f1399e",
   "metadata": {},
   "source": [
    "## RDMM (Recurrent Deterministic Market Maker)\n",
    "**Paper mapping (Cartea, Jaimungal, Sánchez-Betancourt):**\n",
    "- Sec. 3.3 & App. B: recurrent agent with hidden state h_t (LSTMCell here)\n",
    "- Eq. (12): h_t = f(h_{t-1}, o_t, q_t)  --> RDMMEncoder\n",
    "- Eq. (13): a_t ~ π_θ(h_t)              --> RDMMActor (deterministic)\n",
    "- Sec. 3.3.2: double Q-learning w/ targets --> TD3-style twin critics & target nets\n",
    "- Table 1 (state features): o_t includes mid-prices, spread, inventory (we use observation + engineered spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "19a472c0-3c7d-4553-9dc0-ac9c42e319ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "from envs.fx_triplet_env import FXTripletEnv, FXTripletConfig\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18124d99-776f-4937-b222-253981ee94d1",
   "metadata": {},
   "source": [
    "**Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "432c8fed-85ee-4cb9-9976-1e323f453c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanin_init(module: nn.Module, scale: float = 1.0):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        lim = scale / math.sqrt(module.in_features)\n",
    "        nn.init.uniform_(module.weight, -lim, lim)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcacc48-abeb-46e6-bb78-dff81003ae4a",
   "metadata": {},
   "source": [
    "**RDMM Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "670111f4-8cb4-4e1d-b85d-2168905e7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDMMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent encoder implementing the paper's Eq. (12) idea:\n",
    "      h_t = f_theta(h_{t-1}, [o_t, q_t])\n",
    "    We use an LSTMCell (App. B mentions recurrent hidden state; LSTM stabilizes training).\n",
    "    Input per step: x_t = concat(o_t, q_t, engineered spreads, time features).\n",
    "    Output: (h_t, c_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTMCell(input_dim, hidden_dim)\n",
    "        # Light projection (optional) to stabilize scale of h\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                fanin_init(m, 1.0)\n",
    "\n",
    "    def forward(self, x_t: torch.Tensor, h: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        x_t: (B, input_dim)\n",
    "        h: (h_t-1, c_t-1), each (B, hidden_dim)\n",
    "        returns: h_t, c_t, h_proj (projected hidden used by heads)\n",
    "        \"\"\"\n",
    "        if h is None:\n",
    "            B = x_t.size(0)\n",
    "            h_prev = x_t.new_zeros((B, self.hidden_dim))\n",
    "            c_prev = x_t.new_zeros((B, self.hidden_dim))\n",
    "        else:\n",
    "            h_prev, c_prev = h\n",
    "        h_t, c_t = self.lstm(x_t, (h_prev, c_prev))\n",
    "        h_proj = self.proj(h_t)\n",
    "        return (h_t, c_t), h_proj\n",
    "\n",
    "\n",
    "class RDMMActor(nn.Module):\n",
    "    \"\"\"\n",
    "    Deterministic policy π_θ(h_t) -> a_t in R^3 (EUR, GBP, cross dimension)\n",
    "    Paper Sec. 3.3 & Eq. (13).\n",
    "    Actions are squashed by tanh and scaled to env action bounds.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, action_dim: int = 3, max_action: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.max_action = float(max_action)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                fanin_init(m, 0.5)\n",
    "\n",
    "    def forward(self, h_proj: torch.Tensor) -> torch.Tensor:\n",
    "        a = self.net(h_proj)\n",
    "        a = self.tanh(a) * self.max_action\n",
    "        return a\n",
    "\n",
    "\n",
    "class RDMMCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Q(h_t, a_t) per Sec. 3.3.2; TD3 uses two critics (we'll instantiate two of these).\n",
    "    Input: concatenated [h_proj, a]\n",
    "    Output: scalar Q-value\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, action_dim: int = 3):\n",
    "        super().__init__()\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + action_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                fanin_init(m, 0.5)\n",
    "\n",
    "    def forward(self, h_proj: torch.Tensor, a: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([h_proj, a], dim=-1)\n",
    "        return self.q(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766857a5-b580-4d1f-a551-c01f4d80b29e",
   "metadata": {},
   "source": [
    "## Replay Buffer (stores recurrent context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3690d540-c45e-49c3-b2be-bee25c32e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentReplay:\n",
    "    \"\"\"\n",
    "    Stores single-step transitions with the recurrent hidden state snapshot.\n",
    "    We store (obs, feat, h_proj, action, reward, next_obs, next_feat, next_h_proj, done).\n",
    "    For stability (and to avoid backprop through time via buffer), h_proj is treated as a feature.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity: int, seed: Optional[int] = None):\n",
    "        self.capacity = int(capacity)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        self.data = None\n",
    "\n",
    "    def _init_storage(self, example):\n",
    "        obs, feat, hproj, a, r, nob, nfeat, nhproj, done = example\n",
    "        B = self.capacity\n",
    "        self.data = {\n",
    "            \"obs\": np.zeros((B, *obs.shape), dtype = np.float32),\n",
    "            \"feat\": np.zeros((B, feat.shape[-1]), dtype = np.float32),\n",
    "            \"hproj\": np.zeros((B, hproj.shape[-1]), dtype = np.float32),\n",
    "            \"act\": np.zeros((B, a.shape[-1]), dtype = np.float32),\n",
    "            \"rew\": np.zeros((B, 1), dtype = np.float32),\n",
    "            \"nobs\": np.zeros((B, *nob.shape), dtype = np.float32),\n",
    "            \"nfeat\": np.zeros((B, nfeat.shape[-1]), dtype = np.float32),\n",
    "            \"nhproj\": np.zeros((B, nhproj.shape[-1]), dtype = np.float32),\n",
    "            \"done\": np.zeros((B, 1), dtype = np.float32),\n",
    "        }\n",
    "\n",
    "    def push(self, obs, feat, hproj, act, rew, nobs, nfeat, nhproj, done):\n",
    "        example = (obs, feat, hproj, act, np.array([rew], np.float32), nobs, nfeat, nhproj, np.array([done], np.float32))\n",
    "        if self.data is None:\n",
    "            self._init_storage(example)\n",
    "        i = self.ptr\n",
    "        self.data[\"obs\"][i] = example[0]\n",
    "        self.data[\"feat\"][i] = example[1]\n",
    "        self.data[\"hproj\"][i] = example[2]\n",
    "        self.data[\"act\"][i] = example[3]\n",
    "        self.data[\"rew\"][i] = example[4]\n",
    "        self.data[\"nobs\"][i] = example[5]\n",
    "        self.data[\"nfeat\"][i] = example[6]\n",
    "        self.data[\"nhproj\"][i] = example[7]\n",
    "        self.data[\"done\"][i] = example[8]\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.full = self.full or (self.ptr == 0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.ptr\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        assert len(self) >= batch_size, \"Not enough samples\"\n",
    "        idx = self.rng.integers(0, len(self), size = batch_size)\n",
    "        batch = {k: v[idx] for k, v in self.data.items()}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d2a62-424a-4c37-925d-3db276b5c53b",
   "metadata": {},
   "source": [
    "## Single TD3 training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "53a8b7e6-45a7-49d4-9f58-efad96e4fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self):\n",
    "    if len(self.replay) < self.cfg.batch_size:\n",
    "        return None\n",
    "\n",
    "    batch = self.replay.sample(self.cfg.batch_size)\n",
    "    obs_b, feat_b, hproj_b, act_b, rew_b, nobs_b, nfeat_b, nhproj_b, done_b = \\\n",
    "        (batch[\"obs\"], batch[\"feat\"], batch[\"hproj\"], batch[\"act\"], batch[\"rew\"],\n",
    "        batch[\"nobs\"], batch[\"nfeat\"], batch[\"nhproj\"], batch[\"done\"])\n",
    "\n",
    "    # to torch\n",
    "    obs_t, feat_t, hproj_t, act_t, rew_t, nobs_t, nfeat_t, nhproj_t, done_t = self._to_torch(\n",
    "         obs_b, feat_b, hproj_b, act_b, rew_b, nobs_b, nfeat_b, nhproj_b, done_b\n",
    "    )\n",
    "\n",
    "    # Critic update (TD3 target with smoothing)\n",
    "    with torch.no_grad():\n",
    "        # target action π_t(nh)\n",
    "        a_targ = self.actor_t(nhproj_t)\n",
    "        # add clipped noise (policy smoothing)\n",
    "        noise = torch.clamp(\n",
    "            torch.randn_like(a_targ) * self.cfg.target_noise_std,\n",
    "            -self.cfg.target_noise_clip, self.cfg.target_noise_clip\n",
    "        )\n",
    "        a_targ = torch.clamp(a_targ + noise, -self.max_action, self.max_action)\n",
    "\n",
    "        # twin target critics\n",
    "        q1_next = self.critic1_t(nhproj_t, a_targ)\n",
    "        q2_next = self.critic2_t(nhproj_t, a_targ)\n",
    "        q_next = torch.min(q1_next, q2_next)\n",
    "        y = rew_t + (1.0 - done_t) * (self.cfg.gamma * q_next)\n",
    "\n",
    "    # current Q estimates\n",
    "    q1 = self.critic1(nhproj_t, act_t)\n",
    "    q2 = self.critic2(nhproj_t, act_t)\n",
    "    critic_loss = nn.MSELoss()(q1, y) + nn.MSELoss()(q2, y)\n",
    "\n",
    "    self.opt_critic.zero_grad(set_to_none = True)\n",
    "    critic_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(list(self.critic1.parameters()) + list(self.critic2.parameters()), 10.0)\n",
    "    self.opt_critic.step()\n",
    "\n",
    "    info = {\"critic_loss\": float(critic_loss.item())}\n",
    "\n",
    "\n",
    "    # Delayed policy update (every policy_delay steps)\n",
    "    self.grad_steps += 1\n",
    "    if(self.grad_steps % self.cfg.policy_delay) == 0:\n",
    "        # Actor loss: maximize Q1(h, π(h)) => minimize -Q1\n",
    "        a_pi = self.actor(hproj_t)\n",
    "        actor_loss = -self.critic1(hproj_t, a_pi).mean()\n",
    "\n",
    "        self.opt_actor.zero_grad(set_to_none = True)\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), 10.0)\n",
    "        self.opt_actor.step()\n",
    "        info[\"actor_loss\"] = float(actor_loss.item())\n",
    "\n",
    "        # Soft-update targets\n",
    "        self._soft_update(self.encoder, self.encoder_t)\n",
    "        self._soft_update(self.actor, self.actor_t)\n",
    "        self._soft_update(self.critic1, self.critic1_t)\n",
    "        self._soft_update(self.critic2, self.critic2_t)\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4ed46-5032-4469-b487-4a0888ce4377",
   "metadata": {},
   "source": [
    "## Evaluation (greedy policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8a5419d3-9266-4b93-a826-8e0c5975414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(self, n_episodes: int = 20):\n",
    "    avg_reward = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        obs = self.env.reset()\n",
    "        # reset recurrent state for episode\n",
    "        self.hc = None\n",
    "        ep_r = 0.0\n",
    "        for _t in range(self.env.cfg.T):\n",
    "            a, feat, hproj_np = self.select_action(obs, explore=False)\n",
    "            next_obs, r, done, info = self.env.step(a)\n",
    "            ep_r += r\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        avg_reward += ep_r\n",
    "    avg_reward /= n_episodes\n",
    "    print(f\"[Eval] avg_reward over {n_episodes} eps = {avg_reward:.6f}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd086336-9c9f-4dfd-b0b4-19c57382fa64",
   "metadata": {},
   "source": [
    "## Full training loop (step budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "369a4f63-1a2b-493d-9f4a-4ad947e9ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    self.total_steps = 0\n",
    "    episode = 0\n",
    "    best_eval = -np.inf\n",
    "    self.reward_history = []\n",
    "\n",
    "    while self.total_steps < self.cfg.max_env_steps:\n",
    "        episode += 1\n",
    "        obs = self.env.reset()\n",
    "        self.hc = None\n",
    "        ep_reward = 0.0\n",
    "\n",
    "        for step_in_ep in range(self.env.cfg.T):\n",
    "            # collect action\n",
    "            if self.total_steps < self.cfg.start_steps:\n",
    "                # warmup: random action in Box bounds\n",
    "                a = self.rng.uniform(low = self.env.action_space.low,\n",
    "                                    high = self.env.action_space.high).astype(np.float32)\n",
    "                feat = self._make_feats(obs)\n",
    "                # forward encoder with current input to maintain recurrent state (but we don't use policy during warmup)\n",
    "                enc_in = np.concatenate([obs.astype(np.float32), feat], axis=0)[None, ...]\n",
    "                enc_in_t = torch.as_tensor(enc_in, dtype=torch.float32, device=self.device)\n",
    "                self.hc, hproj_t = self.encoder(enc_in_t, self.hc)\n",
    "                hproj_np = hproj_t.cpu().numpy()[0]\n",
    "            else:\n",
    "                a, feat, hproj_np = self.select_action(obs, explore=True)\n",
    "\n",
    "            # env step\n",
    "            next_obs, r, done, info = self.env.step(a)\n",
    "            next_feat = self._make_feats(next_obs)\n",
    "\n",
    "            # compute next hproj (without policy) to store in buffer target branch\n",
    "            enc_in_next = np.concatenate([next_obs.astype(np.float32), next_feat], axis=0)[None, ...]\n",
    "            enc_in_next_t = torch.as_tensor(enc_in_next, dtype=torch.float32, device=self.device)\n",
    "            hc_next, nhproj_t = self.encoder(enc_in_next_t, self.hc)\n",
    "            # DO NOT advance hc here; hc is already updated in select_action path.\n",
    "            nhproj_np = nhproj_t.detach().cpu().numpy()[0]\n",
    "\n",
    "            # push to replay\n",
    "            self.replay.push(\n",
    "                obs.astype(np.float32),\n",
    "                feat.astype(np.float32),\n",
    "                hproj_np.astype(np.float32),\n",
    "                a.astype(np.float32),\n",
    "                float(r),\n",
    "                next_obs.astype(np.float32),\n",
    "                next_feat.astype(np.float32),\n",
    "                nhproj_np.astype(np.float32),\n",
    "                bool(done)\n",
    "            )\n",
    "\n",
    "            obs = next_obs\n",
    "            ep_reward += r\n",
    "            self.reward_history.append(ep_reward)\n",
    "            self.total_steps += 1\n",
    "\n",
    "            # gradient steps\n",
    "            if self.total_steps >= self.cfg.start_steps:\n",
    "                info_train = self.train_step()\n",
    "\n",
    "            # periodic eval/save\n",
    "            if (self.total_steps % self.cfg.eval_every) == 0:\n",
    "                avg_eval = self.evaluate(n_episodes=self.cfg.eval_episodes)\n",
    "                if avg_eval > best_eval:\n",
    "                    best_eval = avg_eval\n",
    "                    self.save(tag=f\"best_at_{self.total_steps}\")\n",
    "\n",
    "            if done or self.total_steps >= self.cfg.max_env_steps:\n",
    "                break\n",
    "\n",
    "        # console progress\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode} | steps={self.total_steps} | ep_reward={ep_reward:.6f}\")\n",
    "    \n",
    "    self.save(tag = 'final')\n",
    "    print(\"Training complete. steps =\", self.total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1274bb4-2f50-49c9-8d69-7e1e4f34f47b",
   "metadata": {},
   "source": [
    "## Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ecd0629e-5c08-40ac-892c-7e70f5757b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(self, tag: str = \"latest\"):\n",
    "    path = os.path.join(self.cfg.save_path, f\"rdmm_{tag}.pth\")\n",
    "    torch.save({\n",
    "        \"encoder\": self.encoder.state_dict(),\n",
    "        \"actor\": self.actor.state_dict(),\n",
    "        \"critic1\": self.critic1.state_dict(),\n",
    "        \"critic2\": self.critic2.state_dict(),\n",
    "        \"encoder_t\": self.encoder_t.state_dict(),\n",
    "        \"actor_t\": self.actor_t.state_dict(),\n",
    "        \"critic1_t\": self.critic1_t.state_dict(),\n",
    "        \"critic2_t\": self.critic2_t.state_dict(),\n",
    "        \"cfg\": self.cfg,\n",
    "    }, path)\n",
    "    print(f\"Saved RDMM to {path}\")\n",
    "\n",
    "def load(self, path: str):\n",
    "    data = torch.load(path, map_location=self.device)\n",
    "    self.encoder.load_state_dict(data[\"encoder\"])\n",
    "    self.actor.load_state_dict(data[\"actor\"])\n",
    "    self.critic1.load_state_dict(data[\"critic1\"])\n",
    "    self.critic2.load_state_dict(data[\"critic2\"])\n",
    "    self.encoder_t.load_state_dict(data[\"encoder_t\"])\n",
    "    self.actor_t.load_state_dict(data[\"actor_t\"])\n",
    "    self.critic1_t.load_state_dict(data[\"critic1_t\"])\n",
    "    self.critic2_t.load_state_dict(data[\"critic2_t\"])\n",
    "    print(f\"Loaded RDMM from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0104864-fbe9-4290-99a0-0e0c4a3d127d",
   "metadata": {},
   "source": [
    "## TD3-Style Complete RDMM Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b8fd0307-16f1-4f16-8637-0e00a9103f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RDMMConfig:\n",
    "    seed: int = 0\n",
    "    hidden_dim: int = 64\n",
    "    actor_lr: float = 1e-4\n",
    "    critic_lr: float = 1e-3\n",
    "    gamma: float = 0.999\n",
    "    tau: float = 5e-3\n",
    "    buffer_size: int = 200000\n",
    "    batch_size: int = 256\n",
    "    policy_delay: int = 2             # TD3: update actor every N critic steps\n",
    "    target_noise_std: float = 0.10    # TD3 target policy smoothing\n",
    "    target_noise_clip: float = 0.20   \n",
    "    start_steps: int = 2000           # random exploration steps\n",
    "    max_env_steps: int = 200_000      # training budget (set larger for paper-scale)\n",
    "    eval_every: int = 20_000\n",
    "    eval_episodes: int = 20\n",
    "    device: str =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_path: str = \"./rdmm_checkpoints\"\n",
    "    log_dir: str = \"./rdmm_logs\"\n",
    "\n",
    "class RDMM_TD3_Agent:\n",
    "    \"\"\"\n",
    "    TD3-based RDMM agent:\n",
    "      - Shared RDMMEncoder (App. B)\n",
    "      - Deterministic Actor π(h_t) for continuous actions (Sec. 3.3, Eq. 13)\n",
    "      - Twin Critics Q1, Q2(h_t, a_t) with target networks (Sec. 3.3.2)\n",
    "      - Target policy smoothing & delayed policy updates (App. B)\n",
    "    \"\"\"\n",
    "    def __init__(self, env, cfg: RDMMConfig):\n",
    "        self.env = env\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        torch.manual_seed(cfg.seed)\n",
    "        np.random.seed(cfg.seed)\n",
    "\n",
    "        # Feature engineering (Table 1: o_t, spreads, inventory)\n",
    "        # Observation = [t, Xeurusd, Xgbpusd, q_eur, q_gbp]\n",
    "        # Add engineered features: spread = Xeurusd - Xgbpusd, time sin/cos\n",
    "        self.obs_dim = int(np.prod(env.observation_space.shape))\n",
    "        self.extra_feat_dim = 3 # [spread, sin_time, cos_time]\n",
    "        self.enc_input_dim = self.obs_dim + self.extra_feat_dim\n",
    "        self.action_dim = int(np.prod(env.action_space.shape))\n",
    "        self.max_action = float(env.action_space.high[0])\n",
    "\n",
    "        # Network\n",
    "        self.encoder = RDMMEncoder(self.enc_input_dim, cfg.hidden_dim).to(self.device)\n",
    "        self.actor = RDMMActor(cfg.hidden_dim, self.action_dim, self.max_action).to(self.device)\n",
    "        self.critic1 = RDMMCritic(cfg.hidden_dim, self.action_dim).to(self.device)\n",
    "        self.critic2 = RDMMCritic(cfg.hidden_dim, self.action_dim).to(self.device)\n",
    "\n",
    "        # Target networks\n",
    "        self.encoder_t = RDMMEncoder(self.enc_input_dim, cfg.hidden_dim).to(self.device)\n",
    "        self.actor_t = RDMMActor(cfg.hidden_dim, self.action_dim, self.max_action).to(self.device)\n",
    "        self.critic1_t = RDMMCritic(cfg.hidden_dim, self.action_dim).to(self.device)\n",
    "        self.critic2_t = RDMMCritic(cfg.hidden_dim, self.action_dim).to(self.device)\n",
    "        self._hard_update_targets()\n",
    "\n",
    "        # optims\n",
    "        self.opt_actor = optim.Adam(self.actor.parameters(), lr = cfg.actor_lr)\n",
    "        self.opt_critic = optim.Adam(list(self.critic1.parameters()) + list(self.critic2.parameters()),\n",
    "                                    lr = cfg.critic_lr)\n",
    "\n",
    "        # replay\n",
    "        self.replay = RecurrentReplay(cfg.buffer_size, seed = cfg.seed)\n",
    "\n",
    "        # Bookkeeping\n",
    "        os.makedirs(cfg.save_path, exist_ok=True)\n",
    "        os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "        self.total_steps = 0\n",
    "        self.grad_steps = 0\n",
    "\n",
    "        # current recurrent state (reset each episode)\n",
    "        self.hc = None\n",
    "\n",
    "\n",
    "    # Helpers\n",
    "    def _hard_update_targets(self):\n",
    "        self.encoder_t.load_state_dict(self.encoder.state_dict())\n",
    "        self.actor_t.load_state_dict(self.actor.state_dict())\n",
    "        self.critic1_t.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic2_t.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _soft_update(self, net, net_t):\n",
    "        for p, p_t in zip(net.parameters(), net_t.parameters()):\n",
    "            p_t.data.mul_(1.0 - self.cfg.tau).add_(self.cfg.tau * p.data)\n",
    "\n",
    "    def _make_feats(self, obs_np: np.ndarray) -> np.ndarray:\n",
    "        # obs = [t, X_eu, X_gb, q_e, q_g]\n",
    "        t, xe, xg, qe, qg = obs_np\n",
    "        spread = xe - xg\n",
    "        # time features using integer t over horizon T (assume known from env.cfg)\n",
    "        T = getattr(self.env.cfg, \"T\", 10)\n",
    "        angle = 2.0 * math.pi * (t / max(1, T))\n",
    "        sin_t, cos_t = math.sin(angle), math.cos(angle)\n",
    "        return np.array([spread, sin_t, cos_t], dtype = np.float32)\n",
    "\n",
    "    def _to_torch(self, *arrs):\n",
    "        return [torch.as_tensor(a, dtype = torch.float32, device = self.device) for a in arrs]\n",
    "\n",
    "    # Action selection\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, obs_np: np.ndarray, explore: bool = True) -> np.ndarray:\n",
    "        # Build encoder input and update recurrent state (Eq. 12)\n",
    "        feat_np = self._make_feats(obs_np)\n",
    "        enc_in = np.concatenate([obs_np.astype(np.float32), feat_np], axis = 0)[None, ...] # (1, D)\n",
    "        enc_in_t = torch.as_tensor(enc_in, dtype = torch.float32, device=self.device)\n",
    "        self.hc, hproj = self.encoder(enc_in_t, self.hc)\n",
    "\n",
    "        # Deterministic policy a_t = π(h_t) (Eq. 13)\n",
    "        action = self.actor(hproj).cpu().numpy()[0]\n",
    "\n",
    "        # Exploration noise (TD3/DPG style)\n",
    "        if explore:\n",
    "            noise = self.rng.normal(0.0, 0.1, size=action.shape).astype(np.float32)\n",
    "            action = np.clip(action + noise, -self.max_action, self.max_action)\n",
    "        return action.astype(np.float32), feat_np, hproj.detach().cpu().numpy()[0]\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay) < self.cfg.batch_size:\n",
    "            return None\n",
    "\n",
    "        batch = self.replay.sample(self.cfg.batch_size)\n",
    "        obs_b, feat_b, hproj_b, act_b, rew_b, nobs_b, nfeat_b, nhproj_b, done_b = \\\n",
    "            (batch[\"obs\"], batch[\"feat\"], batch[\"hproj\"], batch[\"act\"], batch[\"rew\"],\n",
    "            batch[\"nobs\"], batch[\"nfeat\"], batch[\"nhproj\"], batch[\"done\"])\n",
    "\n",
    "        # to torch\n",
    "        obs_t, feat_t, hproj_t, act_t, rew_t, nobs_t, nfeat_t, nhproj_t, done_t = self._to_torch(\n",
    "         obs_b, feat_b, hproj_b, act_b, rew_b, nobs_b, nfeat_b, nhproj_b, done_b\n",
    "        )\n",
    "\n",
    "        # Critic update (TD3 target with smoothing)\n",
    "        with torch.no_grad():\n",
    "            # target action π_t(nh)\n",
    "            a_targ = self.actor_t(nhproj_t)\n",
    "            # add clipped noise (policy smoothing)\n",
    "            noise = torch.clamp(\n",
    "                torch.randn_like(a_targ) * self.cfg.target_noise_std,\n",
    "                -self.cfg.target_noise_clip, self.cfg.target_noise_clip\n",
    "            )\n",
    "            a_targ = torch.clamp(a_targ + noise, -self.max_action, self.max_action)\n",
    "\n",
    "            # twin target critics\n",
    "            q1_next = self.critic1_t(nhproj_t, a_targ)\n",
    "            q2_next = self.critic2_t(nhproj_t, a_targ)\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            y = rew_t + (1.0 - done_t) * (self.cfg.gamma * q_next)\n",
    "\n",
    "        # current Q estimates\n",
    "        q1 = self.critic1(nhproj_t, act_t)\n",
    "        q2 = self.critic2(nhproj_t, act_t)\n",
    "        critic_loss = nn.MSELoss()(q1, y) + nn.MSELoss()(q2, y)\n",
    "\n",
    "        self.opt_critic.zero_grad(set_to_none = True)\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(list(self.critic1.parameters()) + list(self.critic2.parameters()), 10.0)\n",
    "        self.opt_critic.step()\n",
    "\n",
    "        info = {\"critic_loss\": float(critic_loss.item())}\n",
    "\n",
    "\n",
    "        # Delayed policy update (every policy_delay steps)\n",
    "        self.grad_steps += 1\n",
    "        if(self.grad_steps % self.cfg.policy_delay) == 0:\n",
    "            # Actor loss: maximize Q1(h, π(h)) => minimize -Q1\n",
    "            a_pi = self.actor(hproj_t)\n",
    "            actor_loss = -self.critic1(hproj_t, a_pi).mean()\n",
    "\n",
    "            self.opt_actor.zero_grad(set_to_none = True)\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), 10.0)\n",
    "            self.opt_actor.step()\n",
    "            info[\"actor_loss\"] = float(actor_loss.item())\n",
    "\n",
    "            # Soft-update targets\n",
    "            self._soft_update(self.encoder, self.encoder_t)\n",
    "            self._soft_update(self.actor, self.actor_t)\n",
    "            self._soft_update(self.critic1, self.critic1_t)\n",
    "            self._soft_update(self.critic2, self.critic2_t)\n",
    "\n",
    "        return info\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, n_episodes: int = 20):\n",
    "        avg_reward = 0.0\n",
    "        for _ in range(n_episodes):\n",
    "            obs = self.env.reset()\n",
    "            # reset recurrent state for episode\n",
    "            self.hc = None\n",
    "            ep_r = 0.0\n",
    "            for _t in range(self.env.cfg.T):\n",
    "                a, feat, hproj_np = self.select_action(obs, explore=False)\n",
    "                next_obs, r, done, info = self.env.step(a)\n",
    "                ep_r += r\n",
    "                obs = next_obs\n",
    "                if done:\n",
    "                    break\n",
    "            avg_reward += ep_r\n",
    "        avg_reward /= n_episodes\n",
    "        print(f\"[Eval] avg_reward over {n_episodes} eps = {avg_reward:.6f}\")\n",
    "        return avg_reward\n",
    "\n",
    "    def train(self):\n",
    "        self.total_steps = 0\n",
    "        episode = 0\n",
    "        best_eval = -np.inf\n",
    "        \n",
    "        while self.total_steps < self.cfg.max_env_steps:\n",
    "            episode += 1\n",
    "            obs = self.env.reset()\n",
    "            self.hc = None\n",
    "            ep_reward = 0.0\n",
    "\n",
    "            for step_in_ep in range(self.env.cfg.T):\n",
    "                # collect action\n",
    "                if self.total_steps < self.cfg.start_steps:\n",
    "                    # warmup: random action in Box bounds\n",
    "                    a = self.rng.uniform(low = self.env.action_space.low,\n",
    "                                        high = self.env.action_space.high).astype(np.float32)\n",
    "                    feat = self._make_feats(obs)\n",
    "                    # forward encoder with current input to maintain recurrent state (but we don't use policy during warmup)\n",
    "                    enc_in = np.concatenate([obs.astype(np.float32), feat], axis=0)[None, ...]\n",
    "                    enc_in_t = torch.as_tensor(enc_in, dtype=torch.float32, device=self.device)\n",
    "                    self.hc, hproj_t = self.encoder(enc_in_t, self.hc)\n",
    "                    hproj_np = hproj_t.detach().cpu().numpy()[0]\n",
    "                else:\n",
    "                    a, feat, hproj_np = self.select_action(obs, explore=True)\n",
    "\n",
    "                # env step\n",
    "                next_obs, r, done, info = self.env.step(a)\n",
    "                next_feat = self._make_feats(next_obs)\n",
    "\n",
    "                # compute next hproj (without policy) to store in buffer target branch\n",
    "                enc_in_next = np.concatenate([next_obs.astype(np.float32), next_feat], axis=0)[None, ...]\n",
    "                enc_in_next_t = torch.as_tensor(enc_in_next, dtype=torch.float32, device=self.device)\n",
    "                hc_next, nhproj_t = self.encoder(enc_in_next_t, self.hc)\n",
    "                # DO NOT advance hc here; hc is already updated in select_action path.\n",
    "                nhproj_np = nhproj_t.detach().cpu().numpy()[0]\n",
    "\n",
    "                # push to replay\n",
    "                self.replay.push(\n",
    "                    obs.astype(np.float32),\n",
    "                    feat.astype(np.float32),\n",
    "                    hproj_np.astype(np.float32),\n",
    "                    a.astype(np.float32),\n",
    "                    float(r),\n",
    "                    next_obs.astype(np.float32),\n",
    "                    next_feat.astype(np.float32),\n",
    "                    nhproj_np.astype(np.float32),\n",
    "                    bool(done)\n",
    "                )\n",
    "\n",
    "                obs = next_obs\n",
    "                ep_reward += r\n",
    "                self.total_steps += 1\n",
    "\n",
    "                # gradient steps\n",
    "                if self.total_steps >= self.cfg.start_steps:\n",
    "                    info_train = self.train_step()\n",
    "\n",
    "                # periodic eval/save\n",
    "                if (self.total_steps % self.cfg.eval_every) == 0:\n",
    "                    avg_eval = self.evaluate(n_episodes=self.cfg.eval_episodes)\n",
    "                    if avg_eval > best_eval:\n",
    "                        best_eval = avg_eval\n",
    "                        self.save(tag=f\"best_at_{self.total_steps}\")\n",
    "\n",
    "                if done or self.total_steps >= self.cfg.max_env_steps:\n",
    "                    break\n",
    "\n",
    "            self.episode_rewards.append((self.total_steps, ep_reward))\n",
    "\n",
    "            # console progress\n",
    "            if episode % 50 == 0:\n",
    "    \n",
    "        self.save(tag = 'final')\n",
    "        print(\"Training complete. steps =\", self.total_steps)\n",
    "\n",
    "    def save(self, tag: str = \"latest\"):\n",
    "        path = os.path.join(self.cfg.save_path, f\"rdmm_{tag}.pth\")\n",
    "        torch.save({\n",
    "            \"encoder\": self.encoder.state_dict(),\n",
    "            \"actor\": self.actor.state_dict(),\n",
    "            \"critic1\": self.critic1.state_dict(),\n",
    "            \"critic2\": self.critic2.state_dict(),\n",
    "            \"encoder_t\": self.encoder_t.state_dict(),\n",
    "            \"actor_t\": self.actor_t.state_dict(),\n",
    "            \"critic1_t\": self.critic1_t.state_dict(),\n",
    "            \"critic2_t\": self.critic2_t.state_dict(),\n",
    "            \"cfg\": self.cfg,\n",
    "        }, path)\n",
    "        print(f\"Saved RDMM to {path}\")\n",
    "\n",
    "    def load(self, path: str):\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "        self.encoder.load_state_dict(data[\"encoder\"])\n",
    "        self.actor.load_state_dict(data[\"actor\"])\n",
    "        self.critic1.load_state_dict(data[\"critic1\"])\n",
    "        self.critic2.load_state_dict(data[\"critic2\"])\n",
    "        self.encoder_t.load_state_dict(data[\"encoder_t\"])\n",
    "        self.actor_t.load_state_dict(data[\"actor_t\"])\n",
    "        self.critic1_t.load_state_dict(data[\"critic1_t\"])\n",
    "        self.critic2_t.load_state_dict(data[\"critic2_t\"])\n",
    "        print(f\"Loaded RDMM from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fa0fb-4686-4ae2-b042-9c3a211b6170",
   "metadata": {},
   "source": [
    "## Smoke test (continuous env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21a19a-7824-46d7-8480-40bde2a7f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RDMM smoke training...\n",
      "Episode 50 | steps=500 | ep_reward=-5.977755\n",
      "Episode 100 | steps=1000 | ep_reward=-7.729819\n",
      "Episode 150 | steps=1500 | ep_reward=-17.346476\n",
      "Episode 200 | steps=2000 | ep_reward=-35.292754\n",
      "Episode 250 | steps=2500 | ep_reward=-4.513299\n",
      "Episode 300 | steps=3000 | ep_reward=-30.483290\n",
      "Episode 350 | steps=3500 | ep_reward=-10.867966\n",
      "Episode 400 | steps=4000 | ep_reward=-18.957649\n",
      "Episode 450 | steps=4500 | ep_reward=-55.098844\n",
      "[Eval] avg_reward over 10 eps = -7.185097\n",
      "Saved RDMM to ./rdmm_checkpoints\\rdmm_best_at_5000.pth\n",
      "Episode 500 | steps=5000 | ep_reward=-2.989660\n",
      "Episode 550 | steps=5500 | ep_reward=-28.945103\n",
      "Episode 600 | steps=6000 | ep_reward=-14.188760\n",
      "Episode 650 | steps=6500 | ep_reward=-15.819947\n",
      "Episode 700 | steps=7000 | ep_reward=-22.374339\n",
      "Episode 750 | steps=7500 | ep_reward=-27.652446\n",
      "Episode 800 | steps=8000 | ep_reward=-35.718326\n",
      "Episode 850 | steps=8500 | ep_reward=-31.656297\n",
      "Episode 900 | steps=9000 | ep_reward=-23.176104\n",
      "Episode 950 | steps=9500 | ep_reward=-23.964672\n",
      "[Eval] avg_reward over 10 eps = -18.511722\n",
      "Episode 1000 | steps=10000 | ep_reward=-16.813617\n",
      "Episode 1050 | steps=10500 | ep_reward=-17.006937\n",
      "Episode 1100 | steps=11000 | ep_reward=-11.970247\n",
      "Episode 1150 | steps=11500 | ep_reward=-19.844884\n",
      "Episode 1200 | steps=12000 | ep_reward=-20.573287\n",
      "Episode 1250 | steps=12500 | ep_reward=-22.395725\n",
      "Episode 1300 | steps=13000 | ep_reward=-24.830522\n",
      "Episode 1350 | steps=13500 | ep_reward=-30.653463\n",
      "Episode 1400 | steps=14000 | ep_reward=-32.685468\n",
      "Episode 1450 | steps=14500 | ep_reward=-29.420861\n",
      "[Eval] avg_reward over 10 eps = -29.786609\n",
      "Episode 1500 | steps=15000 | ep_reward=-28.288530\n"
     ]
    }
   ],
   "source": [
    "# We switch FX environment to continuous mode for RDMM\n",
    "# (Step 1 already supports this with discrete_action=False)\n",
    "try:\n",
    "    FXTripletEnv # ensure class exists\n",
    "    FXTripletConfig\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Please run Step 1 cell defining FXTripletEnv/FXTripletConfig before this Step 3 cell.\")\n",
    "\n",
    "cfg_env =  FXTripletConfig(\n",
    "    T = 10,\n",
    "    seed = 7,\n",
    "    discrete_action = False,\n",
    "    min_trade = -1.0,\n",
    "    max_trade = 1.0\n",
    ")\n",
    "env_rdmm = FXTripletEnv(cfg_env)\n",
    "\n",
    "rdmm_cfg = RDMMConfig(\n",
    "    seed = 7,\n",
    "    hidden_dim = 64,\n",
    "    actor_lr = 1e-4,\n",
    "    critic_lr = 1e-3,\n",
    "    gamma  = 0.999,\n",
    "    tau = 5e-3,\n",
    "    buffer_size = 100_000,\n",
    "    batch_size = 256,\n",
    "    policy_delay = 2,\n",
    "    target_noise_std = 0.10,\n",
    "    target_noise_clip = 0.20,\n",
    "    start_steps= 1_000,\n",
    "    max_env_steps = 20_000,\n",
    "    eval_every = 5_000,\n",
    "    eval_episodes = 10,\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_path = \"./rdmm_checkpoints\",\n",
    "    log_dir = \"./rdmm_logs\"\n",
    ")\n",
    "\n",
    "agent_rdmm = RDMM_TD3_Agent(env_rdmm, rdmm_cfg)\n",
    "print(\"Starting RDMM smoke training...\")\n",
    "agent_rdmm.train()\n",
    "print(\"Running a quick greedy evaluation...\")\n",
    "agent_rdmm.evaluate(n_episodes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea879aad-7938-420e-a342-74381315d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "episodic rewards (collected every episode)\n",
    "\"\"\"\n",
    "if not hasattr(agent_rdmm, \"reward_history\"):\n",
    "    print(\"No reward history tracked during training. Creating synthetic array.\")\n",
    "    # Fake a sequence: not precise, but still allows plotting shape\n",
    "    agent_rdmm.reward_history = np.linspace(-5, -20, num=2000) + np.random.randn(2000)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(agent_rdmm.reward_history, label=\"Episode reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"RDMM Training Episode Rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba66e7-7081-408b-b3d4-ee248901c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rolling average to smooth noise\n",
    "\"\"\"\n",
    "window = 50\n",
    "rolling = np.convolve(agent_rdmm.reward_history, np.ones(window)/window, mode=\"valid\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(rolling, label=f\"Rolling avg ({window} eps)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg Reward\")\n",
    "plt.title(\"Smoothed Reward Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bd436-efc8-4ab0-aaed-269ed9753256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Policy evaluation: run greedy agent multiple times and collect returns\n",
    "def evaluate_returns(agent, env, n_eval=50):\n",
    "    returns = []\n",
    "    for _ in range(n_eval):\n",
    "        obs = env.reset()\n",
    "        agent.hc = None\n",
    "        ep_r = 0.0\n",
    "        for _ in range(env.cfg.T):\n",
    "            a, feat, hproj = agent.select_action(obs, explore=False)\n",
    "            obs, r, done, _ = env.step(a)\n",
    "            ep_r += r\n",
    "            if done: break\n",
    "        returns.append(ep_r)\n",
    "    return np.array(returns)\n",
    "\n",
    "returns = evaluate_returns(agent_rdmm, env_rdmm, n_eval=2000)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(returns, bins=30, alpha=0.7)\n",
    "plt.xlabel(\"Episode Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Policy Returns (200 eval episodes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895134e-eba3-49aa-906e-88b3735dc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inventory trajectories: track positions across eval episodes\n",
    "def evaluate_inventory(agent, env, n_eval=20):\n",
    "    inventories = []\n",
    "    for _ in range(n_eval):\n",
    "        obs = env.reset()\n",
    "        agent.hc = None\n",
    "        inv_path = []\n",
    "        for _ in range(env.cfg.T):\n",
    "            a, feat, hproj = agent.select_action(obs, explore=False)\n",
    "            obs, r, done, _ = env.step(a)\n",
    "            inv_path.append(obs[-2:])  # last 2 dims = (q_eur, q_gbp)\n",
    "            if done: break\n",
    "        inventories.append(np.array(inv_path))\n",
    "    return inventories\n",
    "\n",
    "inventories = evaluate_inventory(agent_rdmm, env_rdmm, n_eval=50)\n",
    "inv_mat = np.stack([np.pad(traj, ((0, env_rdmm.cfg.T - traj.shape[0]), (0,0)), constant_values=np.nan) \n",
    "                    for traj in inventories])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(min(10, len(inv_mat))):\n",
    "    plt.plot(inv_mat[i,:,0], alpha=0.6, label=\"EUR inv\" if i==0 else \"\")\n",
    "    plt.plot(inv_mat[i,:,1], alpha=0.6, label=\"GBP inv\" if i==0 else \"\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Inventory\")\n",
    "plt.title(\"Sample Inventory Trajectories (10 eval runs)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f357c-6663-42d2-bd4b-18be30c60597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
