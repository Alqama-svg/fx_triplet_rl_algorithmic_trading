{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6ebfef-5ac1-42ee-90d0-9b0bcc396091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c93b89-d226-432b-b56b-9d56e75f4529",
   "metadata": {},
   "source": [
    "## Step 1: FX Triplet Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177b067e-69ea-4825-ba19-0ecfb319246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FXTripletConfig:\n",
    "    T: int = 10                   # horizon (timesteps per episode)\n",
    "    dt: float = 1/252             # timestep length (years)\n",
    "    kappa: float = 5.0            # mean reversion speed\n",
    "    sigma: float = 0.01           # volatility\n",
    "    theta: float = 0.0            # long-run mean of spread\n",
    "    seed: int = 0\n",
    "    discrete_action: bool = True\n",
    "    n_discrete: int = 125\n",
    "    min_trade: float = -1.0       # min units per action dim\n",
    "    max_trade: float = 1.0        # max units per action dim\n",
    "    terminal_penalty: float = 0.0\n",
    "\n",
    "class FXTripletEnv(gym.Env):\n",
    "    def __init__(self, cfg: FXTripletConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        self.t = 0\n",
    "        self.q_eur = 0.0\n",
    "        self.q_gbp = 0.0\n",
    "        self.state = None\n",
    "        # discrete or continuous action space\n",
    "        if cfg.discrete_action:\n",
    "            self._action_table = self._build_action_table(cfg.n_discrete)\n",
    "            self.action_space = gym.spaces.Discrete(len(self._action_table))\n",
    "        else:\n",
    "            self._action_table = None\n",
    "            self.action_space = gym.spaces.Box(\n",
    "                low=cfg.min_trade, high=cfg.max_trade, shape=(3,), dtype=np.float32\n",
    "            )\n",
    "        # observation space: t, Xeurusd, Xgbpusd, q_eur, q_gbp\n",
    "        obs_high = np.array([np.inf]*5, dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(-obs_high, obs_high, dtype=np.float32)\n",
    "\n",
    "    def _build_action_table(self, n: int) -> np.ndarray:\n",
    "        grid_per_dim = int(round(n ** (1/3)))\n",
    "        grid = np.linspace(self.cfg.min_trade, self.cfg.max_trade, grid_per_dim)\n",
    "        combos = np.array(np.meshgrid(grid, grid, grid)).T.reshape(-1,3)\n",
    "        return combos.astype(np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.q_eur = 0.0\n",
    "        self.q_gbp = 0.0\n",
    "        self.Xeurusd = 1.0\n",
    "        self.Xgbpusd = 1.3\n",
    "        self._update_Xeur_gbp()\n",
    "        self.state = np.array([self.t, self.Xeurusd, self.Xgbpusd, self.q_eur, self.q_gbp], dtype=np.float32)\n",
    "        return self.state\n",
    "\n",
    "    def _update_Xeur_gbp(self):\n",
    "        # enforce no-arbitrage: Xeur_gbp = Xeur_usd / Xgbp_usd\n",
    "        self.Xeurgbp = self.Xeurusd / self.Xgbpusd\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.cfg.discrete_action:\n",
    "            a = self._action_table[action]\n",
    "        else:\n",
    "            a = np.clip(action, self.cfg.min_trade, self.cfg.max_trade)\n",
    "        # update inventories\n",
    "        self.q_eur += a[0]\n",
    "        self.q_gbp += a[1]\n",
    "        # simulate cointegrated FX rates\n",
    "        spread = self.Xeurusd - self.Xgbpusd\n",
    "        spread += self.cfg.kappa * (self.cfg.theta - spread) * self.cfg.dt \\\n",
    "                  + self.cfg.sigma * np.sqrt(self.cfg.dt) * self.rng.normal()\n",
    "        self.Xeurusd += self.cfg.sigma * np.sqrt(self.cfg.dt) * self.rng.normal()\n",
    "        self.Xgbpusd = self.Xeurusd - spread\n",
    "        self._update_Xeur_gbp()\n",
    "        # reward: simple P&L from inventory times price change\n",
    "        pnl = (self.q_eur * (self.Xeurusd - self.state[1])) \\\n",
    "            + (self.q_gbp * (self.Xgbpusd - self.state[2]))\n",
    "        reward = pnl\n",
    "        self.t += 1\n",
    "        done = self.t >= self.cfg.T\n",
    "        if done and self.cfg.terminal_penalty != 0:\n",
    "            reward -= self.cfg.terminal_penalty * (self.q_eur**2 + self.q_gbp**2)\n",
    "        self.state = np.array([self.t, self.Xeurusd, self.Xgbpusd, self.q_eur, self.q_gbp], dtype=np.float32)\n",
    "        return self.state, reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b3257-efff-4073-9b2f-d4ecc2727b1d",
   "metadata": {},
   "source": [
    "## Step 2: DDQN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b422198-23ab-473d-bdfd-ecc249613763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alqama\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DDQNConfig:\n",
    "    seed: int = 0\n",
    "    total_episodes: int = 500\n",
    "    max_steps_per_episode: int = 10\n",
    "    batch_size: int = 64\n",
    "    buffer_size: int = 10000\n",
    "    gamma: float = 0.999\n",
    "    lr: float = 1e-4\n",
    "    target_update_freq: int = 100\n",
    "    start_training_after: int = 500\n",
    "    train_frequency: int = 1\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_final: float = 0.01\n",
    "    epsilon_decay_steps: int = 20000\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_path: str = \"./ddqn_checkpoints\"\n",
    "    log_dir: str = \"./ddqn_logs\"\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, seed: Optional[int] = None):\n",
    "        self.capacity = int(capacity)\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "        self.rng = random.Random(seed)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = self.rng.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class MLPQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_sizes: List[int] = [64,64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, action_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, env, cfg: DDQNConfig, hidden_sizes: List[int] = [64,64]):\n",
    "        self.env = env\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.state_dim = int(np.prod(env.observation_space.shape))\n",
    "        self.discrete = True\n",
    "        self.n_actions = int(env.action_space.n)\n",
    "        self.discrete_action_table = env._action_table\n",
    "        self.policy_net = MLPQNetwork(self.state_dim, self.n_actions, hidden_sizes).to(self.device)\n",
    "        self.target_net = MLPQNetwork(self.state_dim, self.n_actions, hidden_sizes).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "        self.replay = ReplayBuffer(cfg.buffer_size, seed=cfg.seed)\n",
    "        self.total_steps = 0\n",
    "        self.train_steps = 0\n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.epsilon_decay = (cfg.epsilon_start - cfg.epsilon_final) / float(max(1, cfg.epsilon_decay_steps))\n",
    "        os.makedirs(cfg.save_path, exist_ok=True)\n",
    "        os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "        self.writer = SummaryWriter(cfg.log_dir)\n",
    "\n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        if not eval_mode and self.rng.random() < self.epsilon:\n",
    "            return int(self.rng.integers(0, self.n_actions))\n",
    "        state_t = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.policy_net(state_t)\n",
    "            action = int(torch.argmax(qvals, dim=1).cpu().item())\n",
    "        return action\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state.astype(np.float32), int(action), float(reward), next_state.astype(np.float32), bool(done))\n",
    "\n",
    "    def _compute_td_loss(self, batch: Transition) -> torch.Tensor:\n",
    "        states = torch.tensor(np.stack(batch.state), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(batch.action, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch.reward, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.stack(batch.next_state), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(batch.done, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_policy = self.policy_net(next_states)\n",
    "            next_actions = torch.argmax(next_q_policy, dim=1, keepdim=True)\n",
    "            next_q_target = self.target_net(next_states).gather(1, next_actions)\n",
    "            td_target = rewards + (1.0 - dones) * (self.cfg.gamma * next_q_target)\n",
    "        loss = nn.MSELoss()(q_values, td_target)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay) < max(self.cfg.batch_size, 1):\n",
    "            return None\n",
    "        batch = self.replay.sample(self.cfg.batch_size)\n",
    "        loss = self._compute_td_loss(batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        self.train_steps += 1\n",
    "        if (self.train_steps % self.cfg.target_update_freq) == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return float(loss.item())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.cfg.epsilon_final:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            if self.epsilon < self.cfg.epsilon_final:\n",
    "                self.epsilon = self.cfg.epsilon_final\n",
    "\n",
    "    def save(self, tag=\"latest\"):\n",
    "        path = os.path.join(self.cfg.save_path, f\"ddqn_{tag}.pth\")\n",
    "        torch.save({\n",
    "            \"policy_state_dict\": self.policy_net.state_dict(),\n",
    "            \"target_state_dict\": self.target_net.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"cfg\": self.cfg,\n",
    "        }, path)\n",
    "        print(f\"Saved model to {path}\")\n",
    "\n",
    "    def evaluate(self, n_episodes=10) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                next_state, r, done, _ = self.env.step(idx)\n",
    "                s = next_state\n",
    "                ep_r += r\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg\n",
    "\n",
    "    def train(self, save_every_episodes=50, eval_every_episodes=50, eval_episodes=10):\n",
    "        total_steps = 0\n",
    "        episode_rewards = []\n",
    "        for episode in range(1, self.cfg.total_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            ep_reward = 0.0\n",
    "            for step in range(self.cfg.max_steps_per_episode):\n",
    "                action_idx = self.select_action(state)\n",
    "                env_action = self.env._action_table[action_idx]\n",
    "                next_state, reward, done, _ = self.env.step(action_idx)\n",
    "                self.push_transition(state, action_idx, reward, next_state, done)\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "                total_steps += 1\n",
    "                if total_steps >= self.cfg.start_training_after:\n",
    "                    loss = self.train_step()\n",
    "                self.decay_epsilon()\n",
    "                if done:\n",
    "                    break\n",
    "            episode_rewards.append(ep_reward)\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Episode {episode}/{self.cfg.total_episodes}, reward={ep_reward:.4f}\")\n",
    "            if episode % save_every_episodes == 0:\n",
    "                self.save(tag=f\"ep{episode}\")\n",
    "            if episode % eval_every_episodes == 0:\n",
    "                self.evaluate(n_episodes=eval_episodes)\n",
    "        self.save(tag=\"final\")\n",
    "        return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4483e-46d1-47f8-851f-7b1fc5113e84",
   "metadata": {},
   "source": [
    "## Instantiate env and run smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0548043b-a4c8-4049-8e25-929cd3ac62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/20, reward=-0.0428\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep10.pth\n",
      "Evaluation over 3 episodes: avg_reward=0.009175\n",
      "Episode 20/20, reward=-0.0946\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep20.pth\n",
      "Evaluation over 3 episodes: avg_reward=0.000913\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_final.pth\n",
      "Smoke test completed. Sample rewards: [-0.0007598812272111566, -0.06326159995140479, -0.08597486848850022, -0.005957135051971707, 0.028583438885849577]\n"
     ]
    }
   ],
   "source": [
    "cfg_env = FXTripletConfig(T=10, seed=0, discrete_action=True, n_discrete=27)\n",
    "env = FXTripletEnv(cfg_env)\n",
    "\n",
    "cfg_ddqn = DDQNConfig(total_episodes=20, max_steps_per_episode=cfg_env.T,\n",
    "                      buffer_size=2000, start_training_after=20, epsilon_decay_steps=1000)\n",
    "\n",
    "agent = DDQNAgent(env, cfg_ddqn)\n",
    "rewards = agent.train(save_every_episodes=10, eval_every_episodes=10, eval_episodes=3)\n",
    "print(\"Smoke test completed. Sample rewards:\", rewards[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805aa35-5afe-4bf2-bddd-f30e8bdb54d2",
   "metadata": {},
   "source": [
    "## Full DDQN training loop using paper hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe01882-2237-4659-9e9d-58b22d9d73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DDQN training (step-based). Device: cpu\n",
      "Episode 100, env_steps=1000, ep_reward=-0.145280, recent_avg=-0.050702, eps=0.5050\n",
      "Episode 200, env_steps=2000, ep_reward=-0.086817, recent_avg=-0.072764, eps=0.0100\n",
      "Episode 300, env_steps=3000, ep_reward=0.111768, recent_avg=-0.038652, eps=0.0100\n",
      "Episode 400, env_steps=4000, ep_reward=0.278558, recent_avg=0.062729, eps=0.0100\n",
      "Episode 500, env_steps=5000, ep_reward=0.283289, recent_avg=0.131062, eps=0.0100\n",
      "Episode 600, env_steps=6000, ep_reward=0.242962, recent_avg=0.203086, eps=0.0100\n",
      "Episode 700, env_steps=7000, ep_reward=0.288316, recent_avg=0.258583, eps=0.0100\n",
      "Episode 800, env_steps=8000, ep_reward=0.253486, recent_avg=0.268208, eps=0.0100\n",
      "Episode 900, env_steps=9000, ep_reward=0.276468, recent_avg=0.261289, eps=0.0100\n",
      "Episode 1000, env_steps=10000, ep_reward=0.226523, recent_avg=0.260121, eps=0.0100\n",
      "Episode 1100, env_steps=11000, ep_reward=0.243352, recent_avg=0.261383, eps=0.0100\n",
      "Episode 1200, env_steps=12000, ep_reward=0.291734, recent_avg=0.259867, eps=0.0100\n",
      "Episode 1300, env_steps=13000, ep_reward=0.236974, recent_avg=0.263813, eps=0.0100\n",
      "Episode 1400, env_steps=14000, ep_reward=0.304482, recent_avg=0.260827, eps=0.0100\n",
      "Episode 1500, env_steps=15000, ep_reward=0.204743, recent_avg=0.261842, eps=0.0100\n",
      "Episode 1600, env_steps=16000, ep_reward=0.161471, recent_avg=0.268271, eps=0.0100\n",
      "Episode 1700, env_steps=17000, ep_reward=0.210211, recent_avg=0.259047, eps=0.0100\n",
      "Episode 1800, env_steps=18000, ep_reward=0.185732, recent_avg=0.246127, eps=0.0100\n",
      "Episode 1900, env_steps=19000, ep_reward=0.265006, recent_avg=0.246149, eps=0.0100\n",
      "Episode 2000, env_steps=20000, ep_reward=0.186794, recent_avg=0.260837, eps=0.0100\n",
      "Episode 2100, env_steps=21000, ep_reward=0.245058, recent_avg=0.279742, eps=0.0100\n",
      "Episode 2200, env_steps=22000, ep_reward=0.261106, recent_avg=0.280076, eps=0.0100\n",
      "Episode 2300, env_steps=23000, ep_reward=0.245571, recent_avg=0.274898, eps=0.0100\n",
      "Episode 2400, env_steps=24000, ep_reward=0.281660, recent_avg=0.273317, eps=0.0100\n",
      "Episode 2500, env_steps=25000, ep_reward=0.298593, recent_avg=0.272840, eps=0.0100\n",
      "Episode 2600, env_steps=26000, ep_reward=0.318757, recent_avg=0.265184, eps=0.0100\n",
      "Episode 2700, env_steps=27000, ep_reward=0.309253, recent_avg=0.265358, eps=0.0100\n",
      "Episode 2800, env_steps=28000, ep_reward=0.275465, recent_avg=0.276575, eps=0.0100\n",
      "Episode 2900, env_steps=29000, ep_reward=0.218015, recent_avg=0.277947, eps=0.0100\n",
      "Episode 3000, env_steps=30000, ep_reward=0.282890, recent_avg=0.276466, eps=0.0100\n",
      "Episode 3100, env_steps=31000, ep_reward=0.266854, recent_avg=0.274688, eps=0.0100\n",
      "Episode 3200, env_steps=32000, ep_reward=0.221561, recent_avg=0.269876, eps=0.0100\n",
      "Episode 3300, env_steps=33000, ep_reward=0.286696, recent_avg=0.274359, eps=0.0100\n",
      "Episode 3400, env_steps=34000, ep_reward=0.211093, recent_avg=0.277874, eps=0.0100\n",
      "Episode 3500, env_steps=35000, ep_reward=0.301527, recent_avg=0.274081, eps=0.0100\n",
      "Episode 3600, env_steps=36000, ep_reward=0.294566, recent_avg=0.271970, eps=0.0100\n",
      "Episode 3700, env_steps=37000, ep_reward=0.298186, recent_avg=0.266689, eps=0.0100\n",
      "Episode 3800, env_steps=38000, ep_reward=0.287416, recent_avg=0.265106, eps=0.0100\n",
      "Episode 3900, env_steps=39000, ep_reward=0.311217, recent_avg=0.268706, eps=0.0100\n",
      "Episode 4000, env_steps=40000, ep_reward=0.250461, recent_avg=0.275337, eps=0.0100\n",
      "Episode 4100, env_steps=41000, ep_reward=0.298045, recent_avg=0.271661, eps=0.0100\n",
      "Episode 4200, env_steps=42000, ep_reward=0.286543, recent_avg=0.272065, eps=0.0100\n",
      "Episode 4300, env_steps=43000, ep_reward=0.305620, recent_avg=0.279583, eps=0.0100\n",
      "Episode 4400, env_steps=44000, ep_reward=0.208710, recent_avg=0.272152, eps=0.0100\n",
      "Episode 4500, env_steps=45000, ep_reward=0.226511, recent_avg=0.254383, eps=0.0100\n",
      "Episode 4600, env_steps=46000, ep_reward=0.260240, recent_avg=0.261523, eps=0.0100\n",
      "Episode 4700, env_steps=47000, ep_reward=0.287108, recent_avg=0.276055, eps=0.0100\n",
      "Episode 4800, env_steps=48000, ep_reward=0.265752, recent_avg=0.272447, eps=0.0100\n",
      "Episode 4900, env_steps=49000, ep_reward=0.269351, recent_avg=0.272176, eps=0.0100\n",
      "Evaluation over 100 episodes: avg_reward=0.258505\n",
      "Saved model to ./ddqn_checkpoints_paper\\ddqn_best_at_50000.pth\n",
      "Episode 5000, env_steps=50000, ep_reward=0.251700, recent_avg=0.273472, eps=0.0100\n",
      "Episode 5100, env_steps=51000, ep_reward=0.256650, recent_avg=0.276603, eps=0.0100\n",
      "Episode 5200, env_steps=52000, ep_reward=0.286122, recent_avg=0.278263, eps=0.0100\n",
      "Episode 5300, env_steps=53000, ep_reward=0.199029, recent_avg=0.273909, eps=0.0100\n",
      "Episode 5400, env_steps=54000, ep_reward=0.310990, recent_avg=0.253855, eps=0.0100\n",
      "Episode 5500, env_steps=55000, ep_reward=0.193920, recent_avg=0.256088, eps=0.0100\n",
      "Episode 5600, env_steps=56000, ep_reward=0.270482, recent_avg=0.276245, eps=0.0100\n",
      "Episode 5700, env_steps=57000, ep_reward=0.284096, recent_avg=0.281467, eps=0.0100\n",
      "Episode 5800, env_steps=58000, ep_reward=0.262540, recent_avg=0.279704, eps=0.0100\n",
      "Episode 5900, env_steps=59000, ep_reward=0.257064, recent_avg=0.279278, eps=0.0100\n",
      "Episode 6000, env_steps=60000, ep_reward=0.196868, recent_avg=0.277260, eps=0.0100\n",
      "Episode 6100, env_steps=61000, ep_reward=0.300422, recent_avg=0.272123, eps=0.0100\n",
      "Episode 6200, env_steps=62000, ep_reward=0.290837, recent_avg=0.279803, eps=0.0100\n",
      "Episode 6300, env_steps=63000, ep_reward=0.271952, recent_avg=0.281173, eps=0.0100\n",
      "Episode 6400, env_steps=64000, ep_reward=0.236052, recent_avg=0.276525, eps=0.0100\n",
      "Episode 6500, env_steps=65000, ep_reward=0.254134, recent_avg=0.264462, eps=0.0100\n",
      "Episode 6600, env_steps=66000, ep_reward=0.301445, recent_avg=0.263266, eps=0.0100\n",
      "Episode 6700, env_steps=67000, ep_reward=0.312119, recent_avg=0.273056, eps=0.0100\n",
      "Episode 6800, env_steps=68000, ep_reward=0.260283, recent_avg=0.269882, eps=0.0100\n",
      "Episode 6900, env_steps=69000, ep_reward=0.314354, recent_avg=0.274201, eps=0.0100\n",
      "Episode 7000, env_steps=70000, ep_reward=0.291391, recent_avg=0.276659, eps=0.0100\n",
      "Episode 7100, env_steps=71000, ep_reward=0.295756, recent_avg=0.279611, eps=0.0100\n",
      "Episode 7200, env_steps=72000, ep_reward=0.258856, recent_avg=0.280179, eps=0.0100\n",
      "Episode 7300, env_steps=73000, ep_reward=0.302866, recent_avg=0.269011, eps=0.0100\n",
      "Episode 7400, env_steps=74000, ep_reward=0.234113, recent_avg=0.265891, eps=0.0100\n",
      "Episode 7500, env_steps=75000, ep_reward=0.239779, recent_avg=0.273149, eps=0.0100\n",
      "Episode 7600, env_steps=76000, ep_reward=0.286114, recent_avg=0.276842, eps=0.0100\n",
      "Episode 7700, env_steps=77000, ep_reward=0.195823, recent_avg=0.259502, eps=0.0100\n",
      "Episode 7800, env_steps=78000, ep_reward=0.260126, recent_avg=0.249616, eps=0.0100\n",
      "Episode 7900, env_steps=79000, ep_reward=0.275080, recent_avg=0.252954, eps=0.0100\n",
      "Episode 8000, env_steps=80000, ep_reward=0.277187, recent_avg=0.263491, eps=0.0100\n",
      "Episode 8100, env_steps=81000, ep_reward=0.321187, recent_avg=0.277940, eps=0.0100\n",
      "Episode 8200, env_steps=82000, ep_reward=0.299437, recent_avg=0.273163, eps=0.0100\n",
      "Episode 8300, env_steps=83000, ep_reward=0.283085, recent_avg=0.275037, eps=0.0100\n",
      "Episode 8400, env_steps=84000, ep_reward=0.283155, recent_avg=0.274133, eps=0.0100\n",
      "Episode 8500, env_steps=85000, ep_reward=0.258180, recent_avg=0.276128, eps=0.0100\n",
      "Episode 8600, env_steps=86000, ep_reward=0.262838, recent_avg=0.277760, eps=0.0100\n",
      "Episode 8700, env_steps=87000, ep_reward=0.294715, recent_avg=0.272718, eps=0.0100\n",
      "Episode 8800, env_steps=88000, ep_reward=0.239401, recent_avg=0.277725, eps=0.0100\n",
      "Episode 8900, env_steps=89000, ep_reward=0.274057, recent_avg=0.267679, eps=0.0100\n",
      "Episode 9000, env_steps=90000, ep_reward=0.294069, recent_avg=0.263643, eps=0.0100\n",
      "Episode 9100, env_steps=91000, ep_reward=0.284425, recent_avg=0.272104, eps=0.0100\n",
      "Episode 9200, env_steps=92000, ep_reward=0.278477, recent_avg=0.273030, eps=0.0100\n",
      "Episode 9300, env_steps=93000, ep_reward=0.285195, recent_avg=0.273624, eps=0.0100\n",
      "Episode 9400, env_steps=94000, ep_reward=0.275884, recent_avg=0.277416, eps=0.0100\n"
     ]
    }
   ],
   "source": [
    "env_cfg = FXTripletConfig(\n",
    "    T=10,\n",
    "    seed=42,\n",
    "    # important model params set to paper values\n",
    ")\n",
    "# set paper-specific parameters on the env config object if dataclass supports them\n",
    "env_cfg.xbar_eusd = 1.1\n",
    "env_cfg.xbar_gbusd = 1.3\n",
    "env_cfg.kappa_e = 0.5\n",
    "env_cfg.kappa_g = 0.5\n",
    "env_cfg.eta_g_on_e = -0.3\n",
    "env_cfg.eta_e_on_g = -0.3\n",
    "env_cfg.sigma = 0.01\n",
    "env_cfg.min_trade = -1.0\n",
    "env_cfg.max_trade = 1.0\n",
    "env_cfg.phi_eusd = 0.001\n",
    "env_cfg.phi_gbusd = 0.001\n",
    "env_cfg.phi_eurgbp = 0.001\n",
    "env_cfg.alpha_eur = 1.0\n",
    "env_cfg.alpha_gbp = 1.0\n",
    "\n",
    "# instantiate environment with discrete actions for DDQN\n",
    "env = FXTripletEnv(env_cfg)  # uses env._action_table if discrete\n",
    "\n",
    "# DDQN training config (paper-like)\n",
    "cfg = DDQNConfig(\n",
    "    seed=42,\n",
    "    total_episodes=100,           # not used directly; loop by env steps below\n",
    "    max_steps_per_episode=env_cfg.T,  # T=10\n",
    "    batch_size=32,\n",
    "    buffer_size=10000,\n",
    "    gamma=0.999,\n",
    "    lr=1e-3,\n",
    "    target_update_freq=100,           # M = 100 gradient steps\n",
    "    start_training_after=2000,        # fill buffer with some experiences first\n",
    "    train_frequency=1,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_final=0.01,\n",
    "    epsilon_decay_steps=2000,         # fast decay\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_path=\"./ddqn_checkpoints_paper\",\n",
    "    log_dir=f\"./ddqn_logs_paper/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "\n",
    "os.makedirs(cfg.save_path, exist_ok=True)\n",
    "os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "\n",
    "# instantiate agent\n",
    "agent = DDQNAgent(env, cfg, hidden_sizes=[64,64])\n",
    "\n",
    "# step-based training loop (stop after target_env_steps)\n",
    "target_env_steps = 1_000_000    # paper-level learning steps (adjustable)\n",
    "\n",
    "best_eval_reward = -np.inf\n",
    "best_path = None\n",
    "eval_every = 1_000            # evaluate every N env steps\n",
    "eval_episodes = 100           # evaluate over 100 episodes\n",
    "save_every = 100000           # checkpoint freq\n",
    "\n",
    "env_steps = 0\n",
    "episode = 0\n",
    "train_losses = []\n",
    "episode_rewards = []\n",
    "print(\"Starting DDQN training (step-based). Device:\", agent.device)\n",
    "\n",
    "while env_steps < target_env_steps:\n",
    "    episode += 1\n",
    "    state = env.reset()\n",
    "    ep_reward = 0.0\n",
    "    for step_in_ep in range(cfg.max_steps_per_episode):\n",
    "        action_idx = agent.select_action(state, eval_mode=False)\n",
    "        # pass index directly to env since env is Discrete\n",
    "        next_state, reward, done, info = env.step(int(action_idx))\n",
    "        agent.push_transition(state, action_idx, reward, next_state, done)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        env_steps += 1\n",
    "\n",
    "        # training step(s) (train every env step after warmup)\n",
    "        if env_steps >= cfg.start_training_after and (env_steps % cfg.train_frequency == 0):\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                train_losses.append(loss)\n",
    "                agent.writer.add_scalar(\"train/loss\", loss, env_steps)\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        # periodic evaluation\n",
    "        if (env_steps % eval_every) == 0:\n",
    "            eval_avg = agent.evaluate(n_episodes=eval_episodes)\n",
    "            agent.writer.add_scalar(\"eval/avg_reward\", eval_avg, env_steps)\n",
    "            # save model if better\n",
    "            if eval_avg > best_eval_reward:\n",
    "                best_eval_reward = eval_avg\n",
    "                agent.save(tag=f\"best_at_{env_steps}\")\n",
    "        # periodic checkpoint\n",
    "        if (env_steps % save_every) == 0:\n",
    "            agent.save(tag=f\"step_{env_steps}\")\n",
    "\n",
    "        if env_steps >= target_env_steps:\n",
    "            break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "    agent.writer.add_scalar(\"episode/reward\", ep_reward, episode)\n",
    "    # print progress occasionally\n",
    "    if episode % 100 == 0:\n",
    "        recent = np.mean(episode_rewards[-200:]) if len(episode_rewards) >= 1 else ep_reward\n",
    "        print(f\"Episode {episode}, env_steps={env_steps}, ep_reward={ep_reward:.6f}, recent_avg={recent:.6f}, eps={agent.epsilon:.4f}\")\n",
    "\n",
    "# final save\n",
    "agent.save(tag=\"final\")\n",
    "print(\"Training finished. env_steps=\", env_steps)\n",
    "\n",
    "# Save training traces for later plotting\n",
    "np.savez(os.path.join(cfg.save_path, \"train_traces.npz\"),\n",
    "         train_losses=np.array(train_losses),\n",
    "         episode_rewards=np.array(episode_rewards))\n",
    "\n",
    "# optional quick plot of loss and rewards (local)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.title(\"Train loss (MSE)\")\n",
    "plt.xlabel(\"train step\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(episode_rewards, label=\"episode_reward\")\n",
    "plt.title(\"Episode rewards\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbfd2b-3150-4957-a681-ba5a682af2c0",
   "metadata": {},
   "source": [
    "## Evaluation: P&L histogram and inventory quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0115a-66e8-4881-bc73-133e29465646",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 2000    # increase to 10000 for paper-level stats if you have compute\n",
    "T = env.cfg.T\n",
    "\n",
    "pnl_list = []\n",
    "qe_paths = np.zeros((n_sim, T+1))\n",
    "qg_paths = np.zeros((n_sim, T+1))\n",
    "\n",
    "for s in trange(n_sim):\n",
    "    obs = env.reset()\n",
    "    qe_paths[s,0] = env.q_eur\n",
    "    qg_paths[s,0] = env.q_gbp\n",
    "    total_pnl = 0.0\n",
    "    for t in range(T):\n",
    "        # greedy eval\n",
    "        action_idx = agent.select_action(obs, eval_mode=True)\n",
    "        obs, r, done, info = env.step(int(action_idx))\n",
    "        total_pnl += r\n",
    "        qe_paths[s,t+1] = info[\"q_eur\"]\n",
    "        qg_paths[s,t+1] = info[\"q_gbp\"]\n",
    "        if done:\n",
    "            break\n",
    "    pnl_list.append(total_pnl)\n",
    "\n",
    "pnl_arr = np.array(pnl_list)\n",
    "\n",
    "# P&L histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(pnl_arr, bins=80)\n",
    "plt.title(f\"P&L distribution (n={n_sim}) â€” mean {pnl_arr.mean():.4f}, std {pnl_arr.std():.4f}\")\n",
    "plt.xlabel(\"P&L (USD-scaled units)\")\n",
    "plt.show()\n",
    "\n",
    "# inventory quantiles (10%,50%,90%)\n",
    "qe_qs = np.percentile(qe_paths, [10,50,90], axis=0)\n",
    "qg_qs = np.percentile(qg_paths, [10,50,90], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2da2e-1d47-4aaa-999c-b2a8cd5d2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.fill_between(range(T+1), qe_qs[0], qe_qs[2], alpha=0.25)\n",
    "plt.plot(range(T+1), qe_qs[1], label=\"median\")\n",
    "plt.title(\"EUR inventory quantiles\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.fill_between(range(T+1), qg_qs[0], qg_qs[2], alpha=0.25)\n",
    "plt.plot(range(T+1), qg_qs[1], label=\"median\")\n",
    "plt.title(\"GBP inventory quantiles\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34548be1-b7c2-4697-bee0-93bcdbbafce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
