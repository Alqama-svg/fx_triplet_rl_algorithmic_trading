{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "75e7279e-9150-462e-91c8-bbb9fe583e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from envs.fx_triplet_env import FXTripletEnv, FXTripletConfig\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except Exception as e:\n",
    "    raise ImportError(\"PyTorch is required for this DDQN implementation.\"\n",
    "        \"Install it before running this cell. Example (CPU-only):\\n\"\n",
    "        \"  pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\\n\"\n",
    "        \"or see https://pytorch.org/get-started/locally/ for the correct command for environment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81502485-cfdb-45c7-a662-f3b6cb325601",
   "metadata": {},
   "source": [
    "## Config for DDQN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f4c0eccd-ceaf-4ba3-bdc7-14de518df9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DDQNConfig:\n",
    "    # trading\n",
    "    seed: int = 0\n",
    "    total_episodes: int = 500\n",
    "    max_steps_per_episode: int = 10\n",
    "    batch_size: int = 64\n",
    "    buffer_size: int = 10000\n",
    "    gamma: float = 0.999\n",
    "    lr: float = 1e-4\n",
    "    target_update_freq: int = 100      # in gradient steps\n",
    "    start_training_after: int = 500    # steps to fill replay before training\n",
    "    train_frequency: int = 1           # train every N steps\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_final: float = 0.01\n",
    "    epsilon_decay_steps: int = 20000   # linear decay steps\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_path: str = \"./ddqn_checkpoints\"\n",
    "    log_dir:str = \"./ddqn_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bca20b-915c-43f6-b653-e0e371dead69",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "35073766-cd8d-4d71-85a4-8f120752b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, seed: Optional[int] = None):\n",
    "        self.capacity = int(capacity)\n",
    "        self.buffer = deque(maxlen = self.capacity)\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = self.rng.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0acd2-095f-43e0-ac46-ef3eb35e6fdb",
   "metadata": {},
   "source": [
    "## Q-network (simple MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bf7e2831-ee3a-4205-8568-89da16f1e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_sizes: List[int] = [64,64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, action_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608be756-8740-429c-9431-5898adbbcec8",
   "metadata": {},
   "source": [
    "## DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "75c6a2fc-00e8-4e3a-ab8d-6174588054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 cfg: DDQNConfig,\n",
    "                 hidden_sizes: List[int] = [64, 64],\n",
    "                 discrete_action_table: Optional[np.array] = None):\n",
    "        \"\"\"\n",
    "        env: gym-like env implementing Step 1\n",
    "        cfg: DDQNConfig dataclass\n",
    "        discrete_action_table: if env uses discrete actions, provide mapping array (n_actions, 3)\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        self.device = torch.device(cfg.device)\n",
    "        # determine state / action dims\n",
    "        self.state_dim = int(np.prod(env.observation_space.shape))\n",
    "        # For discrete actions, the agent acts over ints 0..n-1\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            # discrete\n",
    "            self.discrete = True\n",
    "            self.n_actions = int(env.action_space.n)\n",
    "            self.action_dim = self.n_actions\n",
    "            if discrete_action_table is None:\n",
    "                self.discrete_action_table = getattr(env, \"_action_table\", None)\n",
    "            else:\n",
    "                self.discrete_action_table = discrete_action_table\n",
    "        else:\n",
    "            self.discrete = False\n",
    "            # continuous actions: discretize to N bins per dimension for DDQN baseline if needed\n",
    "            # But paper uses DDQN with a small discrete action set; here I assume env.action_space is Discrete for DDQN.\n",
    "            raise ValueError(\"DDQN baseline expects a discrete action space in the env. for continous, discrete first\")\n",
    "        # networks\n",
    "        self.policy_net = MLPNetwork(self.state_dim, self.action_dim, hidden_sizes).to(self.device)\n",
    "        self.target_net = MLPNetwork(self.state_dim, self.action_dim, hidden_sizes).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # optimizer & buffer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "        self.replay = ReplayBuffer(cfg.buffer_size, seed=cfg.seed)\n",
    "        self.total_steps = 0\n",
    "        self.train_steps = 0\n",
    "\n",
    "        # epsilon schedule params\n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.epsilon_decay = (cfg.epsilon_start - cfg.epsilon_final) / float(max(1, cfg.epsilon_decay_steps))\n",
    "\n",
    "        # logging\n",
    "        os.makedirs(cfg.save_path, exist_ok=True)\n",
    "        os.makedirs(cfg.log_dir, exist_ok=True)\n",
    "        self.writer = SummaryWriter(cfg.log_dir)\n",
    "\n",
    "    def select_action(self, state: np.ndarray, eval_mode: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        state: np.ndarray shaped like env.observation_space\n",
    "        returns: integer action (index into discrete action table)\n",
    "        \"\"\"\n",
    "        if not eval_mode and self.rng.random() < self.epsilon:\n",
    "            return int(self.rng.integers(0, self.action_dim))\n",
    "        # else greedy\n",
    "        state_t = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.policy_net(state_t)\n",
    "            action = int(torch.argmax(qvals, dim = 1).cpu().item())\n",
    "        return action\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state.astype(np.float32), int(action), float(reward), next_state.astype(np.float32), bool(done))\n",
    "\n",
    "    def _compute_td_loss(self, batch: Transition) -> torch.Tensor:\n",
    "        states = torch.tensor(np.stack(batch.state), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(batch.action, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch.reward, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.stack(batch.next_state), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(batch.done, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        # current Q(s, a)\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "\n",
    "        # Double DQN target: I used policy_net to choose argmax action, target_net to eval\n",
    "        with torch.no_grad():\n",
    "            next_q_policy = self.policy_net(next_states)\n",
    "            next_actions = torch.argmax(next_q_policy, dim=1, keepdim=True)\n",
    "            next_q_target = self.target_net(next_states).gather(1, next_actions)\n",
    "            td_target = rewards + (1.0 - dones) * (self.cfg.gamma * next_q_target)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, td_target)\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self):\n",
    "        # ensuring enough samples\n",
    "        if len(self.replay) < max(self.cfg.batch_size, 1):\n",
    "            return None\n",
    "        batch = self.replay.sample(self.cfg.batch_size)\n",
    "        loss = self._compute_td_loss(batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping for stability\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        self.train_steps += 1\n",
    "        \n",
    "        # target network update\n",
    "        if (self.train_steps % self.cfg.target_update_freq) == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return float(loss.item())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.cfg.epsilon_final:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            if self.epsilon < self.cfg.epsilon_final:\n",
    "                self.epsilon = self.cfg.epsilon_final\n",
    "    \n",
    "    def save(self, tag: str = \"latest\"):\n",
    "        path = os.path.join(self.cfg.save_path, f\"ddqn_{tag}.pth\")\n",
    "        torch.save({\n",
    "            \"policy_state_dict\": self.policy_net.state_dict(),\n",
    "            \"target_state_dict\": self.target_net.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"cfg\": self.cfg,\n",
    "        }, path)\n",
    "        print(f\"Saved model to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "        self.policy_net.load_state_dict(data[\"policy_state_dict\"])\n",
    "        self.target_net.load_state_dict(data[\"target_state_dict\"])\n",
    "        if \"optimizer_state_dict\" in data:\n",
    "            try:\n",
    "                self.optimizer.load_state_dict(data[\"optimizer_state_dict\"])\n",
    "            except Exception:\n",
    "                print(\"Could not load optimizer state (shapes mismatch). Continuing without loading optimizer state\")\n",
    "        print(f\"Loaded model from {path}\")\n",
    "    \n",
    "    def action_index_to_env_action(self, index: int) -> np.ndarray:\n",
    "        if self.discrete_action_table is None:\n",
    "            raise RuntimeError(\"No discrete action table available to map index to environment action\")\n",
    "        return np.array(self.discrete_action_table[int(index)], dtype=np.float32)\n",
    "\n",
    "    # Full training loop (episodic)\n",
    "    def train(self,\n",
    "              save_every_episodes: int = 50,\n",
    "              eval_every_episodes: int = 50,\n",
    "              eval_episodes: int = 10,\n",
    "              verbose: bool = True):\n",
    "        total_steps = 0\n",
    "        episode_rewards = []\n",
    "        for episode in range(1, self.cfg.total_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            ep_reward = 0.0\n",
    "            for step in range(self.cfg.max_steps_per_episode):\n",
    "                # action index selection\n",
    "                action_idx = self.select_action(state, eval_mode=False)\n",
    "                # map to env action values\n",
    "                env_action = self.action_index_to_env_action(action_idx)\n",
    "                next_state, reward, done, info = self.env.step(env_action)\n",
    "                self.push_transition(state, action_idx, reward, next_state, done)\n",
    "                state = next_state\n",
    "                ep_reward += reward\n",
    "                total_steps += 1\n",
    "                # training step\n",
    "                if total_steps >= self.cfg.start_training_after and (total_steps % self.cfg.train_frequency == 0):\n",
    "                    loss = self.train_step()\n",
    "                    if loss is not None:\n",
    "                        self.writer.add_scalar(\"train/loss\", loss, global_step=total_steps)\n",
    "                # epsilon decay\n",
    "                self.decay_epsilon()\n",
    "                if done:\n",
    "                    break\n",
    "            episode_rewards.append(ep_reward)\n",
    "            self.writer.add_scalar(\"episode/reward\", ep_reward, episode)\n",
    "            if verbose and (episode % 10 == 0 or episode == 1):\n",
    "                avg_recent = np.mean(episode_rewards[-50:])\n",
    "                print(f\"Episode {episode}/{self.cfg.total_episodes}   ep_reward={ep_reward:.4f}  eps={self.epsilon:.4f}  avg50={avg_recent:.4f}\")\n",
    "            # save\n",
    "            if episode % save_every_episodes == 0:\n",
    "                self.save(tag=f\"ep{episode}\")\n",
    "            # eval\n",
    "            if episode % eval_every_episodes == 0:\n",
    "                avg_eval = self.evaluate(n_episodes=eval_episodes)\n",
    "                self.writer.add_scalar(\"eval/avg_reward\", avg_eval, episode)\n",
    "        # final save\n",
    "        self.save(tag=\"final\")\n",
    "        return episode_rewards\n",
    "\n",
    "\n",
    "    # Evaluation (greedy policy)\n",
    "    def evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0  # greedy\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                a = self.action_index_to_env_action(idx)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b4cb7-9f21-43d6-b7d5-851d0a897cd1",
   "metadata": {},
   "source": [
    "## Full training loop (episodic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "55ef56b5-a181-417f-ab73-41a3f1526b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,\n",
    "        save_every_episodes: int = 50,\n",
    "        eval_every_episodes: int = 50,\n",
    "        eval_episodes: int = 10,\n",
    "        verbose: bool = True):\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    for episode in range(1, self.cfg.total_episodes + 1):\n",
    "        state = self.env.reset()\n",
    "        ep_reward = 0.0\n",
    "        for step in range(self.cfg.max_steps_per_episode):\n",
    "            # action index selection\n",
    "            action_idx = self.select_action(state, eval_mode=False)\n",
    "            # map to env action values\n",
    "            env_action = self.action_index_to_env_action(action_idx)\n",
    "            next_state, reward, done, info = self.env.step(env_action)\n",
    "            self.push_transition(state, action_idx, reward, next_state, done)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            total_steps += 1\n",
    "            # training step\n",
    "            if total_steps >= self.cfg.start_training_after and (total_steps % self.cfg.train_frequency == 0):\n",
    "                loss = self.train_step()\n",
    "                if loss is not None:\n",
    "                    self.writer.add_scalar(\"train/loss\", loss, global_step=total_steps)\n",
    "            # epsilon decay\n",
    "            self.decay_epsilon()\n",
    "            if done:\n",
    "                break\n",
    "        episode_rewards.append(ep_reward)\n",
    "        self.writer.add_scalar(\"episode/reward\", ep_reward, episode)\n",
    "        if verbose and (episode % 10 == 0 or episode == 1):\n",
    "            avg_recent = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode}/{self.cfg.total_episodes}   ep_reward={ep_reward:.4f}  eps={self.epsilon:.4f}  avg50={avg_recent:.4f}\")\n",
    "        # save\n",
    "        if episode % save_every_episodes == 0:\n",
    "            self.save(tag=f\"ep{episode}\")\n",
    "        # eval\n",
    "        if episode % eval_every_episodes == 0:\n",
    "            avg_eval = self.evaluate(n_episodes=eval_episodes)\n",
    "            self.writer.add_scalar(\"eval/avg_reward\", avg_eval, episode)\n",
    "        # final save\n",
    "        self.save(tag=\"final\")\n",
    "        return episode_rewards\n",
    "\n",
    "    # Evaluation (greedy policy)\n",
    "    def evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0  # greedy\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                a = self.action_index_to_env_action(idx)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf20a6-886d-4aa0-99e0-e924629b4690",
   "metadata": {},
   "source": [
    "## Evaluation (greedy policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9faa3f97-66ef-4af2-b79a-6bad4bbdefe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\\n        old_eps = self.epsilon\\n        self.epsilon = 0.0  # greedy\\n        rewards = []\\n        for ep in range(n_episodes):\\n            s = self.env.reset()\\n            ep_r = 0.0\\n            for _ in range(self.cfg.max_steps_per_episode):\\n                idx = self.select_action(s, eval_mode=True)\\n                a = self.action_index_to_env_action(idx)\\n                s, r, done, _ = self.env.step(a)\\n                ep_r += r\\n                if render:\\n                    self.env.render()\\n                if done:\\n                    break\\n            rewards.append(ep_r)\\n        self.epsilon = old_eps\\n        avg = float(np.mean(rewards))\\n        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\\n        return avg'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def evaluate(self, n_episodes: int = 10, render: bool = False) -> float:\n",
    "        old_eps = self.epsilon\n",
    "        self.epsilon = 0.0  # greedy\n",
    "        rewards = []\n",
    "        for ep in range(n_episodes):\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0.0\n",
    "            for _ in range(self.cfg.max_steps_per_episode):\n",
    "                idx = self.select_action(s, eval_mode=True)\n",
    "                a = self.action_index_to_env_action(idx)\n",
    "                s, r, done, _ = self.env.step(a)\n",
    "                ep_r += r\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_r)\n",
    "        self.epsilon = old_eps\n",
    "        avg = float(np.mean(rewards))\n",
    "        print(f\"Evaluation over {n_episodes} episodes: avg_reward={avg:.6f}\")\n",
    "        return avg\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930aecc-7f43-46df-bf88-84baf519c0d1",
   "metadata": {},
   "source": [
    "## creation of an action table for DDQN using env._action_table as per availablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "72dcf477-d2ff-43ea-abbc-2c784e420e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discrete_table_from_env(env, max_grid_per_dim=5):\n",
    "    \"\"\"\n",
    "    If env has an _action_table attribute, return it.\n",
    "    Otherwise, build a coarse grid over 3 dims with max_grid_per_dim values per dimension.\n",
    "    \"\"\"\n",
    "    if hasattr(env, \"_action_table\") and env._action_table is not None:\n",
    "        return np.array(env._action_table)\n",
    "    # try to inspect action bounds\n",
    "    assert hasattr(env, \"cfg\"), \"Env must expose cfg with min_trade/max_trade\"\n",
    "    minv = env.cfg.min_trade\n",
    "    maxv = env.cfg.max_trade\n",
    "    grid = np.linspace(minv, maxv, max_grid_per_dim)\n",
    "    combos = np.array(np.meshgrid(grid, grid, grid)).T.reshape(-1,3)\n",
    "    # optionally prune duplicates or too many combos\n",
    "    # choose a subset\n",
    "    n = min(len(combos), 125)  # cap to 125\n",
    "    idx = np.linspace(0, len(combos)-1, n).astype(int)\n",
    "    return combos[idx].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366aac74-76ba-44b5-890f-9776e35189ed",
   "metadata": {},
   "source": [
    "## Smoke-test routine (small run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0ba7f466-867e-4ee9-8b44-fb8c513f9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_smoke_test():\n",
    "    print(\"Running DDQN smoke test (very small training run)...\")\n",
    "    # create env if not in global scope\n",
    "    try:\n",
    "        env  # check if env exists\n",
    "    except NameError:\n",
    "        # require user to define env in notebook\n",
    "        raise RuntimeError(\"Please ensure FXTripletEnv from Step 1 is defined and 'env' is instantiated in the notebook scope before running this smoke test.\")\n",
    "    # wrap up\n",
    "    cfg = DDQNConfig(total_episodes=20, max_steps_per_episode=env.cfg.T, buffer_size=2000, start_training_after=20, epsilon_decay_steps=1000)\n",
    "    disc_table = make_discrete_table_from_env(env, max_grid_per_dim=3)\n",
    "    agent = DDQNAgent(env, cfg, hidden_sizes=[64,64], discrete_action_table=disc_table)\n",
    "    # run training for a few episodes\n",
    "    rewards = agent.train(save_every_episodes=10, eval_every_episodes=10, eval_episodes=3, verbose=True)\n",
    "    print(\"Smoke test completed. Sample rewards:\", rewards[:5])\n",
    "    return agent, rewards\n",
    "# agent, rewards = ddqn_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1fa26e30-8b38-4297-a9d4-6618e2a8f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2000   ep_reward=-20.7218  eps=0.9995  avg50=-20.7218\n",
      "Episode 10/2000   ep_reward=-7.3526  eps=0.9953  avg50=-10.5352\n",
      "Episode 20/2000   ep_reward=-13.2742  eps=0.9905  avg50=-11.5995\n",
      "Episode 30/2000   ep_reward=-13.7848  eps=0.9858  avg50=-11.6794\n",
      "Episode 40/2000   ep_reward=-0.7469  eps=0.9810  avg50=-13.2729\n",
      "Episode 50/2000   ep_reward=-25.0618  eps=0.9763  avg50=-14.2149\n",
      "Evaluation over 10 episodes: avg_reward=-156.043839\n",
      "Episode 60/2000   ep_reward=-2.6770  eps=0.9715  avg50=-14.8217\n",
      "Episode 70/2000   ep_reward=-0.7618  eps=0.9668  avg50=-14.2075\n",
      "Episode 80/2000   ep_reward=-3.9934  eps=0.9620  avg50=-14.0919\n",
      "Episode 90/2000   ep_reward=-70.5474  eps=0.9573  avg50=-13.4457\n",
      "Episode 100/2000   ep_reward=-44.1889  eps=0.9525  avg50=-13.7499\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep100.pth\n",
      "Evaluation over 10 episodes: avg_reward=-155.957245\n",
      "Episode 110/2000   ep_reward=-12.1454  eps=0.9478  avg50=-12.6462\n",
      "Episode 120/2000   ep_reward=-25.6958  eps=0.9430  avg50=-15.3723\n",
      "Episode 130/2000   ep_reward=-1.1107  eps=0.9383  avg50=-15.1287\n",
      "Episode 140/2000   ep_reward=-16.5258  eps=0.9335  avg50=-15.2828\n",
      "Episode 150/2000   ep_reward=-5.2367  eps=0.9288  avg50=-13.0124\n",
      "Evaluation over 10 episodes: avg_reward=-7.539783\n",
      "Episode 160/2000   ep_reward=-15.4161  eps=0.9240  avg50=-13.7757\n",
      "Episode 170/2000   ep_reward=-4.5966  eps=0.9193  avg50=-13.9657\n",
      "Episode 180/2000   ep_reward=-25.8738  eps=0.9145  avg50=-17.6983\n",
      "Episode 190/2000   ep_reward=-14.3330  eps=0.9098  avg50=-18.8805\n",
      "Episode 200/2000   ep_reward=-5.7796  eps=0.9050  avg50=-20.8375\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep200.pth\n",
      "Evaluation over 10 episodes: avg_reward=-523.612672\n",
      "Episode 210/2000   ep_reward=-19.2327  eps=0.9003  avg50=-20.7503\n",
      "Episode 220/2000   ep_reward=-28.5926  eps=0.8955  avg50=-21.5835\n",
      "Episode 230/2000   ep_reward=-23.9836  eps=0.8908  avg50=-21.3704\n",
      "Episode 240/2000   ep_reward=-18.4425  eps=0.8860  avg50=-21.9851\n",
      "Episode 250/2000   ep_reward=-5.0629  eps=0.8813  avg50=-20.1707\n",
      "Evaluation over 10 episodes: avg_reward=-123.432029\n",
      "Episode 260/2000   ep_reward=-13.4334  eps=0.8765  avg50=-21.3696\n",
      "Episode 270/2000   ep_reward=-5.5760  eps=0.8718  avg50=-19.8347\n",
      "Episode 280/2000   ep_reward=-34.3231  eps=0.8670  avg50=-18.9326\n",
      "Episode 290/2000   ep_reward=-17.2527  eps=0.8623  avg50=-18.2949\n",
      "Episode 300/2000   ep_reward=-121.7101  eps=0.8575  avg50=-21.9584\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep300.pth\n",
      "Evaluation over 10 episodes: avg_reward=-394.870835\n",
      "Episode 310/2000   ep_reward=-89.9402  eps=0.8528  avg50=-24.6869\n",
      "Episode 320/2000   ep_reward=-2.7338  eps=0.8480  avg50=-26.9014\n",
      "Episode 330/2000   ep_reward=-24.8867  eps=0.8433  avg50=-25.2192\n",
      "Episode 340/2000   ep_reward=-0.7557  eps=0.8385  avg50=-22.4005\n",
      "Episode 350/2000   ep_reward=-16.4868  eps=0.8338  avg50=-19.0144\n",
      "Evaluation over 10 episodes: avg_reward=-12.710772\n",
      "Episode 360/2000   ep_reward=-15.6564  eps=0.8290  avg50=-14.2961\n",
      "Episode 370/2000   ep_reward=-5.5095  eps=0.8243  avg50=-10.2947\n",
      "Episode 380/2000   ep_reward=-12.9729  eps=0.8195  avg50=-13.5779\n",
      "Episode 390/2000   ep_reward=-1.0840  eps=0.8148  avg50=-14.2587\n",
      "Episode 400/2000   ep_reward=-56.5856  eps=0.8100  avg50=-15.8174\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep400.pth\n",
      "Evaluation over 10 episodes: avg_reward=-51.477004\n",
      "Episode 410/2000   ep_reward=-1.3067  eps=0.8053  avg50=-15.4751\n",
      "Episode 420/2000   ep_reward=-25.6702  eps=0.8005  avg50=-18.5922\n",
      "Episode 430/2000   ep_reward=-8.0437  eps=0.7958  avg50=-15.6820\n",
      "Episode 440/2000   ep_reward=-11.1058  eps=0.7910  avg50=-16.0050\n",
      "Episode 450/2000   ep_reward=-3.4481  eps=0.7863  avg50=-18.2898\n",
      "Evaluation over 10 episodes: avg_reward=-32.273300\n",
      "Episode 460/2000   ep_reward=-6.7339  eps=0.7815  avg50=-21.0281\n",
      "Episode 470/2000   ep_reward=-38.8874  eps=0.7768  avg50=-19.2412\n",
      "Episode 480/2000   ep_reward=-10.5293  eps=0.7720  avg50=-19.6261\n",
      "Episode 490/2000   ep_reward=-34.7268  eps=0.7673  avg50=-20.4151\n",
      "Episode 500/2000   ep_reward=-23.9339  eps=0.7625  avg50=-17.1419\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep500.pth\n",
      "Evaluation over 10 episodes: avg_reward=-29.943327\n",
      "Episode 510/2000   ep_reward=-6.8113  eps=0.7578  avg50=-16.5943\n",
      "Episode 520/2000   ep_reward=-33.9075  eps=0.7530  avg50=-15.8749\n",
      "Episode 530/2000   ep_reward=-9.0856  eps=0.7483  avg50=-16.2911\n",
      "Episode 540/2000   ep_reward=-2.0377  eps=0.7435  avg50=-15.5113\n",
      "Episode 550/2000   ep_reward=-58.7951  eps=0.7388  avg50=-17.3051\n",
      "Evaluation over 10 episodes: avg_reward=-15.881870\n",
      "Episode 560/2000   ep_reward=-10.6890  eps=0.7340  avg50=-18.7490\n",
      "Episode 570/2000   ep_reward=-98.7135  eps=0.7293  avg50=-20.7250\n",
      "Episode 580/2000   ep_reward=-2.6953  eps=0.7245  avg50=-18.8958\n",
      "Episode 590/2000   ep_reward=-12.1613  eps=0.7198  avg50=-19.3203\n",
      "Episode 600/2000   ep_reward=-6.3731  eps=0.7150  avg50=-18.5651\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep600.pth\n",
      "Evaluation over 10 episodes: avg_reward=-3.003115\n",
      "Episode 610/2000   ep_reward=-5.2889  eps=0.7103  avg50=-20.2453\n",
      "Episode 620/2000   ep_reward=-28.8277  eps=0.7055  avg50=-18.5593\n",
      "Episode 630/2000   ep_reward=-30.6408  eps=0.7008  avg50=-21.4286\n",
      "Episode 640/2000   ep_reward=-0.0444  eps=0.6960  avg50=-21.6379\n",
      "Episode 650/2000   ep_reward=-1.7334  eps=0.6913  avg50=-22.1312\n",
      "Evaluation over 10 episodes: avg_reward=-105.377138\n",
      "Episode 660/2000   ep_reward=-6.4863  eps=0.6865  avg50=-18.2197\n",
      "Episode 670/2000   ep_reward=-10.6746  eps=0.6818  avg50=-20.9364\n",
      "Episode 680/2000   ep_reward=-12.4253  eps=0.6770  avg50=-18.1618\n",
      "Episode 690/2000   ep_reward=-28.3051  eps=0.6723  avg50=-17.9624\n",
      "Episode 700/2000   ep_reward=-0.4982  eps=0.6675  avg50=-15.4330\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep700.pth\n",
      "Evaluation over 10 episodes: avg_reward=-7.943992\n",
      "Episode 710/2000   ep_reward=-15.7833  eps=0.6628  avg50=-14.4956\n",
      "Episode 720/2000   ep_reward=-18.3388  eps=0.6580  avg50=-10.5930\n",
      "Episode 730/2000   ep_reward=-7.6926  eps=0.6533  avg50=-11.2015\n",
      "Episode 740/2000   ep_reward=-12.9664  eps=0.6485  avg50=-10.5258\n",
      "Episode 750/2000   ep_reward=-0.4686  eps=0.6438  avg50=-10.6780\n",
      "Evaluation over 10 episodes: avg_reward=-0.622619\n",
      "Episode 760/2000   ep_reward=-4.1167  eps=0.6390  avg50=-10.2154\n",
      "Episode 770/2000   ep_reward=-11.3996  eps=0.6343  avg50=-10.9558\n",
      "Episode 780/2000   ep_reward=-1.9741  eps=0.6295  avg50=-9.8295\n",
      "Episode 790/2000   ep_reward=-1.4443  eps=0.6248  avg50=-8.5609\n",
      "Episode 800/2000   ep_reward=-13.1136  eps=0.6200  avg50=-10.1716\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep800.pth\n",
      "Evaluation over 10 episodes: avg_reward=-45.010523\n",
      "Episode 810/2000   ep_reward=-6.1320  eps=0.6153  avg50=-11.7550\n",
      "Episode 820/2000   ep_reward=-4.9786  eps=0.6105  avg50=-10.8222\n",
      "Episode 830/2000   ep_reward=-0.0625  eps=0.6058  avg50=-10.1307\n",
      "Episode 840/2000   ep_reward=-2.3868  eps=0.6010  avg50=-10.5543\n",
      "Episode 850/2000   ep_reward=-7.0005  eps=0.5963  avg50=-8.2782\n",
      "Evaluation over 10 episodes: avg_reward=-5.338285\n",
      "Episode 860/2000   ep_reward=-6.0906  eps=0.5915  avg50=-6.6738\n",
      "Episode 870/2000   ep_reward=-0.1780  eps=0.5868  avg50=-5.7431\n",
      "Episode 880/2000   ep_reward=-2.7692  eps=0.5820  avg50=-5.2257\n",
      "Episode 890/2000   ep_reward=-0.9526  eps=0.5773  avg50=-4.7229\n",
      "Episode 900/2000   ep_reward=-5.0082  eps=0.5725  avg50=-4.9083\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep900.pth\n",
      "Evaluation over 10 episodes: avg_reward=-4.018392\n",
      "Episode 910/2000   ep_reward=-0.0322  eps=0.5678  avg50=-5.3327\n",
      "Episode 920/2000   ep_reward=-7.7388  eps=0.5630  avg50=-6.3788\n",
      "Episode 930/2000   ep_reward=-4.1361  eps=0.5583  avg50=-7.2445\n",
      "Episode 940/2000   ep_reward=-5.7577  eps=0.5535  avg50=-6.8502\n",
      "Episode 950/2000   ep_reward=-3.6179  eps=0.5488  avg50=-6.1095\n",
      "Evaluation over 10 episodes: avg_reward=-3.095654\n",
      "Episode 960/2000   ep_reward=-12.2796  eps=0.5440  avg50=-6.0098\n",
      "Episode 970/2000   ep_reward=-0.4307  eps=0.5393  avg50=-4.7160\n",
      "Episode 980/2000   ep_reward=-0.9736  eps=0.5345  avg50=-4.0671\n",
      "Episode 990/2000   ep_reward=-7.9918  eps=0.5298  avg50=-4.4553\n",
      "Episode 1000/2000   ep_reward=-0.0347  eps=0.5250  avg50=-4.9270\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1000.pth\n",
      "Evaluation over 10 episodes: avg_reward=-3.583439\n",
      "Episode 1010/2000   ep_reward=-0.6529  eps=0.5203  avg50=-4.1122\n",
      "Episode 1020/2000   ep_reward=-5.7883  eps=0.5155  avg50=-4.8597\n",
      "Episode 1030/2000   ep_reward=-5.9238  eps=0.5108  avg50=-5.2330\n",
      "Episode 1040/2000   ep_reward=-13.5417  eps=0.5060  avg50=-6.0576\n",
      "Episode 1050/2000   ep_reward=-0.1514  eps=0.5013  avg50=-7.5561\n",
      "Evaluation over 10 episodes: avg_reward=-21.632544\n",
      "Episode 1060/2000   ep_reward=-1.4315  eps=0.4965  avg50=-7.8760\n",
      "Episode 1070/2000   ep_reward=-2.5114  eps=0.4918  avg50=-7.1578\n",
      "Episode 1080/2000   ep_reward=-5.3849  eps=0.4870  avg50=-6.3580\n",
      "Episode 1090/2000   ep_reward=-1.5640  eps=0.4823  avg50=-5.3624\n",
      "Episode 1100/2000   ep_reward=-27.4423  eps=0.4775  avg50=-4.0740\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1100.pth\n",
      "Evaluation over 10 episodes: avg_reward=-7.236777\n",
      "Episode 1110/2000   ep_reward=-14.6084  eps=0.4728  avg50=-4.5444\n",
      "Episode 1120/2000   ep_reward=-4.9416  eps=0.4680  avg50=-4.8887\n",
      "Episode 1130/2000   ep_reward=-4.0511  eps=0.4633  avg50=-5.7433\n",
      "Episode 1140/2000   ep_reward=-3.5511  eps=0.4585  avg50=-5.6874\n",
      "Episode 1150/2000   ep_reward=-10.3703  eps=0.4538  avg50=-5.0048\n",
      "Evaluation over 10 episodes: avg_reward=-2.161898\n",
      "Episode 1160/2000   ep_reward=-3.4527  eps=0.4490  avg50=-4.7221\n",
      "Episode 1170/2000   ep_reward=-7.4367  eps=0.4443  avg50=-5.3493\n",
      "Episode 1180/2000   ep_reward=-11.0720  eps=0.4395  avg50=-4.8877\n",
      "Episode 1190/2000   ep_reward=-2.8083  eps=0.4348  avg50=-5.6805\n",
      "Episode 1200/2000   ep_reward=-0.9807  eps=0.4300  avg50=-5.8939\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1200.pth\n",
      "Evaluation over 10 episodes: avg_reward=-4.765194\n",
      "Episode 1210/2000   ep_reward=-22.5656  eps=0.4253  avg50=-5.6427\n",
      "Episode 1220/2000   ep_reward=-3.3782  eps=0.4205  avg50=-4.6577\n",
      "Episode 1230/2000   ep_reward=-0.1582  eps=0.4158  avg50=-4.9015\n",
      "Episode 1240/2000   ep_reward=-2.7690  eps=0.4110  avg50=-4.1442\n",
      "Episode 1250/2000   ep_reward=-1.4784  eps=0.4063  avg50=-3.8641\n",
      "Evaluation over 10 episodes: avg_reward=-1.972858\n",
      "Episode 1260/2000   ep_reward=-7.4585  eps=0.4015  avg50=-3.7113\n",
      "Episode 1270/2000   ep_reward=-0.4971  eps=0.3968  avg50=-3.3820\n",
      "Episode 1280/2000   ep_reward=-12.6830  eps=0.3920  avg50=-3.2468\n",
      "Episode 1290/2000   ep_reward=-2.7521  eps=0.3873  avg50=-3.1098\n",
      "Episode 1300/2000   ep_reward=-9.5979  eps=0.3825  avg50=-3.2673\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1300.pth\n",
      "Evaluation over 10 episodes: avg_reward=-2.828639\n",
      "Episode 1310/2000   ep_reward=-1.3429  eps=0.3778  avg50=-2.9582\n",
      "Episode 1320/2000   ep_reward=-1.1553  eps=0.3730  avg50=-3.1811\n",
      "Episode 1330/2000   ep_reward=-3.2573  eps=0.3683  avg50=-3.2115\n",
      "Episode 1340/2000   ep_reward=-5.7964  eps=0.3635  avg50=-3.0892\n",
      "Episode 1350/2000   ep_reward=-14.7094  eps=0.3588  avg50=-3.0410\n",
      "Evaluation over 10 episodes: avg_reward=-0.556493\n",
      "Episode 1360/2000   ep_reward=-1.8585  eps=0.3540  avg50=-2.9035\n",
      "Episode 1370/2000   ep_reward=-1.2357  eps=0.3493  avg50=-3.0387\n",
      "Episode 1380/2000   ep_reward=-1.2177  eps=0.3445  avg50=-2.3558\n",
      "Episode 1390/2000   ep_reward=-0.5806  eps=0.3398  avg50=-2.3540\n",
      "Episode 1400/2000   ep_reward=-3.8601  eps=0.3350  avg50=-2.1295\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1400.pth\n",
      "Evaluation over 10 episodes: avg_reward=-0.740255\n",
      "Episode 1410/2000   ep_reward=-1.4292  eps=0.3303  avg50=-2.0782\n",
      "Episode 1420/2000   ep_reward=-9.0630  eps=0.3255  avg50=-1.9537\n",
      "Episode 1430/2000   ep_reward=-2.8836  eps=0.3208  avg50=-3.0783\n",
      "Episode 1440/2000   ep_reward=-3.8726  eps=0.3160  avg50=-3.1067\n",
      "Episode 1450/2000   ep_reward=-2.5035  eps=0.3113  avg50=-3.3875\n",
      "Evaluation over 10 episodes: avg_reward=-2.469316\n",
      "Episode 1460/2000   ep_reward=-0.0814  eps=0.3065  avg50=-3.8209\n",
      "Episode 1470/2000   ep_reward=-0.0560  eps=0.3018  avg50=-3.4880\n",
      "Episode 1480/2000   ep_reward=-0.1981  eps=0.2970  avg50=-2.2378\n",
      "Episode 1490/2000   ep_reward=-5.8069  eps=0.2923  avg50=-2.5652\n",
      "Episode 1500/2000   ep_reward=-11.2414  eps=0.2875  avg50=-2.6536\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1500.pth\n",
      "Evaluation over 10 episodes: avg_reward=-14.487617\n",
      "Episode 1510/2000   ep_reward=-0.0984  eps=0.2828  avg50=-2.4122\n",
      "Episode 1520/2000   ep_reward=-2.5781  eps=0.2780  avg50=-2.6088\n",
      "Episode 1530/2000   ep_reward=-1.2281  eps=0.2733  avg50=-2.8772\n",
      "Episode 1540/2000   ep_reward=-1.3578  eps=0.2685  avg50=-2.3657\n",
      "Episode 1550/2000   ep_reward=-1.4068  eps=0.2638  avg50=-1.8656\n",
      "Evaluation over 10 episodes: avg_reward=-0.539089\n",
      "Episode 1560/2000   ep_reward=-2.5310  eps=0.2590  avg50=-1.6003\n",
      "Episode 1570/2000   ep_reward=-0.4160  eps=0.2543  avg50=-1.6915\n",
      "Episode 1580/2000   ep_reward=-1.0658  eps=0.2495  avg50=-1.4938\n",
      "Episode 1590/2000   ep_reward=-1.7299  eps=0.2448  avg50=-1.5548\n",
      "Episode 1600/2000   ep_reward=-0.6870  eps=0.2400  avg50=-1.4897\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1600.pth\n",
      "Evaluation over 10 episodes: avg_reward=-0.738618\n",
      "Episode 1610/2000   ep_reward=-3.4581  eps=0.2353  avg50=-1.5470\n",
      "Episode 1620/2000   ep_reward=-1.1552  eps=0.2305  avg50=-1.4429\n",
      "Episode 1630/2000   ep_reward=-0.7773  eps=0.2258  avg50=-1.5020\n",
      "Episode 1640/2000   ep_reward=-1.4222  eps=0.2210  avg50=-1.7539\n",
      "Episode 1650/2000   ep_reward=-1.5329  eps=0.2163  avg50=-1.7831\n",
      "Evaluation over 10 episodes: avg_reward=-0.914143\n",
      "Episode 1660/2000   ep_reward=-0.1992  eps=0.2115  avg50=-1.7486\n",
      "Episode 1670/2000   ep_reward=-1.2062  eps=0.2068  avg50=-1.5602\n",
      "Episode 1680/2000   ep_reward=-1.1931  eps=0.2020  avg50=-1.4463\n",
      "Episode 1690/2000   ep_reward=-1.4959  eps=0.1973  avg50=-1.2388\n",
      "Episode 1700/2000   ep_reward=-1.5550  eps=0.1925  avg50=-1.5354\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1700.pth\n",
      "Evaluation over 10 episodes: avg_reward=-0.385743\n",
      "Episode 1710/2000   ep_reward=-11.0978  eps=0.1878  avg50=-2.0350\n",
      "Episode 1720/2000   ep_reward=-2.0411  eps=0.1830  avg50=-2.2148\n",
      "Episode 1730/2000   ep_reward=-0.0560  eps=0.1783  avg50=-2.3967\n",
      "Episode 1740/2000   ep_reward=-30.3768  eps=0.1735  avg50=-5.3790\n",
      "Episode 1750/2000   ep_reward=-0.4816  eps=0.1688  avg50=-6.2198\n",
      "Evaluation over 10 episodes: avg_reward=-1.360093\n",
      "Episode 1760/2000   ep_reward=-0.1939  eps=0.1640  avg50=-5.6717\n",
      "Episode 1770/2000   ep_reward=-0.6756  eps=0.1593  avg50=-5.4735\n",
      "Episode 1780/2000   ep_reward=-3.4031  eps=0.1545  avg50=-5.3226\n",
      "Episode 1790/2000   ep_reward=-0.9233  eps=0.1498  avg50=-2.2145\n",
      "Episode 1800/2000   ep_reward=-1.8114  eps=0.1450  avg50=-1.1021\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1800.pth\n",
      "Evaluation over 10 episodes: avg_reward=-0.291368\n",
      "Episode 1810/2000   ep_reward=-0.5430  eps=0.1403  avg50=-1.0789\n",
      "Episode 1820/2000   ep_reward=-0.8813  eps=0.1355  avg50=-1.0036\n",
      "Episode 1830/2000   ep_reward=-0.1124  eps=0.1308  avg50=-1.2170\n",
      "Episode 1840/2000   ep_reward=-0.3434  eps=0.1260  avg50=-1.1228\n",
      "Episode 1850/2000   ep_reward=-0.1438  eps=0.1213  avg50=-1.3507\n",
      "Evaluation over 10 episodes: avg_reward=-0.034595\n",
      "Episode 1860/2000   ep_reward=-1.7290  eps=0.1165  avg50=-1.4190\n",
      "Episode 1870/2000   ep_reward=-0.1111  eps=0.1118  avg50=-1.4232\n",
      "Episode 1880/2000   ep_reward=-0.1640  eps=0.1070  avg50=-1.1355\n",
      "Episode 1890/2000   ep_reward=-1.5058  eps=0.1023  avg50=-1.1845\n",
      "Episode 1900/2000   ep_reward=-0.0795  eps=0.0975  avg50=-0.7871\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep1900.pth\n",
      "Evaluation over 10 episodes: avg_reward=-1.041958\n",
      "Episode 1910/2000   ep_reward=-1.7428  eps=0.0928  avg50=-0.7369\n",
      "Episode 1920/2000   ep_reward=-0.3990  eps=0.0880  avg50=-0.6244\n",
      "Episode 1930/2000   ep_reward=-1.0207  eps=0.0833  avg50=-0.5955\n",
      "Episode 1940/2000   ep_reward=-0.0355  eps=0.0785  avg50=-0.6439\n",
      "Episode 1950/2000   ep_reward=-0.4442  eps=0.0738  avg50=-0.7069\n",
      "Evaluation over 10 episodes: avg_reward=-0.482958\n",
      "Episode 1960/2000   ep_reward=-5.3318  eps=0.0690  avg50=-0.8666\n",
      "Episode 1970/2000   ep_reward=-0.3017  eps=0.0643  avg50=-0.9811\n",
      "Episode 1980/2000   ep_reward=-0.5379  eps=0.0595  avg50=-0.9838\n",
      "Episode 1990/2000   ep_reward=-0.1890  eps=0.0548  avg50=-0.8179\n",
      "Episode 2000/2000   ep_reward=-0.3104  eps=0.0500  avg50=-0.8254\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_ep2000.pth\n",
      "Evaluation over 10 episodes: avg_reward=-0.599947\n",
      "Saved model to ./ddqn_checkpoints\\ddqn_final.pth\n",
      "Discrete(125)\n"
     ]
    }
   ],
   "source": [
    "cfg_env = FXTripletConfig(\n",
    "    seed=0,\n",
    "    discrete_action=True,\n",
    "    n_discrete=125\n",
    ")\n",
    "env = FXTripletEnv(cfg_env)\n",
    "\n",
    "cfg = DDQNConfig(\n",
    "    total_episodes=2000, \n",
    "    max_steps_per_episode=env.cfg.T, \n",
    "    buffer_size=50000,\n",
    "    batch_size = 64,\n",
    "    start_training_after=1000,\n",
    "    train_frequency=1,               # update every step\n",
    "    target_update_freq=500,          # slower target updates\n",
    "    epsilon_start=1.0,               # full exploration\n",
    "    epsilon_final=0.05,              # still some exploration at the end\n",
    "    epsilon_decay_steps=20000,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "agent = DDQNAgent(env, cfg, hidden_sizes=[128,128])\n",
    "rewards = agent.train(save_every_episodes=100, eval_every_episodes=50, eval_episodes=10, verbose=True)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "217e6f0e-6444-4180-8663-7a23b314fcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards, label=\"Episode reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
